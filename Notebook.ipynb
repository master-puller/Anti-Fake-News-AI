{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"StudentCopy_Section_2_FakeNews.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3810jvsc74a57bd01367987cc7840cfe11a5e48493d0489ed8a2d67afc5da873c4c87b7776f6181c","display_name":"Python 3.8.10 64-bit ('venv': venv)"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gVgIxW-YPeQH"},"source":["# Fake News Classification\n","\n","\n","### Connect Colab to VSCode\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install kora -q\n","from kora import jupyter\n","jupyter.start(lab=True)\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"source":["## Import Libraries"],"cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qT8bCpXGr2nO"},"source":["import tqdm\n","import importlib\n","#@title Run this code to get started\n","imports = {\n","    'pandas':'pd',\n","    'numpy': 'np',\n","    'matplotlib.pyplot': 'plt',\n","    'seaborn': 'sns',\n","    'sklearn':None,\n","    'string':None,\n","    'nltk':None,\n","    'spacy':None,\n","    'wordcloud':None,\n","    'sys':None,\n","    'os':None}\n","\n","print(\"Importing libraries\")\n","for name in tqdm.tqdm(imports):\n","    try:\n","        alias = imports[name] or name\n","        globals()[alias] = __import__(name)\n","    except ImportError:\n","        print(f\"Failed importing {name}\")\n","\n","# import math\n","# import os\n","# import numpy as np\n","# import pandas as pd\n","\n","# import pickle\n","\n","# import requests, io, zipfile\n","\n","# Download class resources...\n","\n","basepath = './dataset'\n","#basepath = 'drive/MyDrive/Notebooks/Fake_news_data'\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","\n","\n","pd.options.mode.chained_assignment = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Importing NLP\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\lolzc\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\lolzc\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["print(\"Importing NLP\")\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from spacy.lang.en.stop_words import STOP_WORDS\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import accuracy_score\n","#!python -m spacy download en_core_web_md\n","import en_core_web_md\n","text_to_nlp = en_core_web_md.load()"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["# Load Data\n","import datetime\n","from dateutil import parser\n","fake_data = pd.read_csv(os.path.join(basepath, 'Fake.csv'))\n","true_data = pd.read_csv(os.path.join(basepath, 'True.csv'))\n","# Format the data\n","fake_data['fake'] = 1.0\n","true_data['fake'] = 0.0\n","\n","# Convert the date column into machine-readable\n","# def date(val):\n","#     datet : datetime.datetime = parser.parse(val)\n","#     return [datet.year, datet.month, datet.day]\n","\n","# lf = len(fake_data)\n","# lt = len(true_data)\n","\n","# print(f'datapoints: {lt+lf}')\n","# print(f'% fake: {lt/lf * 100}')\n","\n","dataset = pd.concat([fake_data, true_data])\n","del dataset['date']"]},{"source":["# Dataset Views"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Topics:\\n\\t\", *list(set(dataset['subject'])), sep='\\n\\t')\n","\n","dataset.head()\n"]},{"source":["### Generate Word cloud for fake/real news"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fake = True\n","print(\"Fake news Word cloud\")\n","news_text = ''\n","for t in dataset[dataset['fake'] == fake]['text'].values: # form field cell\n","    news_text += t + ' '\n","    \n","wordcloud = WordCloud()    \n","wordcloud.generate_from_text(news_text)\n","plt.figure(figsize=(14,7))\n","plt.imshow(wordcloud, interpolation='bilinear')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fake = False\n","print(\"Real news Word cloud\")\n","news_text = ''\n","for t in dataset[dataset['fake'] == fake]['text'].values: # form field cell\n","    news_text += t + ' '\n","    \n","wordcloud = WordCloud()    \n","wordcloud.generate_from_text(news_text)\n","plt.figure(figsize=(14,7))\n","plt.imshow(wordcloud, interpolation='bilinear')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fake = True\n","print(\"Fake news Title cloud\")\n","news_text = ''\n","for t in dataset[dataset['fake'] == fake]['title'].values: # form field cell\n","    news_text += t + ' '\n","    \n","wordcloud = WordCloud()    \n","wordcloud.generate_from_text(news_text)\n","plt.figure(figsize=(14,7))\n","plt.imshow(wordcloud, interpolation='bilinear')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fake = False\n","print(\"True news Title cloud\")\n","news_text = ''\n","for t in dataset[dataset['fake'] == fake]['title'].values: # form field cell\n","    news_text += t + ' '\n","    \n","wordcloud = WordCloud()    \n","wordcloud.generate_from_text(news_text)\n","plt.figure(figsize=(14,7))\n","plt.imshow(wordcloud, interpolation='bilinear')"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'spacy.tokens.doc.Doc' object has no attribute 'vectors'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m<ipython-input-73-45b168fdc05c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_to_nlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"burger alphabet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'vectors'"]}],"source":["\n"]},{"source":["## Prepare data for NLP"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not \"text_to_nlp\" in globals():\n","    print(\"Loading NLP converter\")\n","    text_to_nlp = en_core_web_md.load() #Prepare Spacy\n","print(\"Tokenizing text\")\n","\n","\n","x_word2vec = np.zeros((len(dataset),), dtype=object)\n","i = 0\n","for text in tqdm.tqdm(dataset['text']):\n","    x_word2vec[i] = text_to_nlp(text) # Array of tokens, passed to model\n","    i += 1\n","\n","\n","# print(\"Tokenizing titles\")\n","# x_titlevec = np.zeros((len(dataset),), dtype=object)\n","# i = 0\n","# for text in tqdm.tqdm(dataset['title']):\n","#     x_titlevec[i] = text_to_nlp(text) # Array of tokens, passed to model\n","#     i += 1\n","# x_titlevec = np.array(x_word2vec)"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["__name__ : 57 B\n__doc__ : 113 B\n__package__ : 16 B\n__loader__ : 16 B\n__spec__ : 16 B\n__builtin__ : 72 B\n__builtins__ : 72 B\n_ih : 776 B\n_oh : 232 B\n_dh : 96 B\nIn : 776 B\nOut : 232 B\nget_ipython : 64 B\nexit : 48 B\nquit : 48 B\n_ : 51 B\n__ : 51 B\n___ : 51 B\nsys : 72 B\nos : 72 B\n_i : 722 B\n_ii : 102 B\n_iii : 270 B\n_i1 : 1036 B\ntqdm : 72 B\nimportlib : 72 B\nimports : 640 B\nname : 51 B\nalias : 51 B\npd : 72 B\nnp : 72 B\nplt : 72 B\nsns : 72 B\nsklearn : 72 B\nstring : 72 B\nnltk : 72 B\nspacy : 72 B\nwordcloud : 72 B\nbasepath : 58 B\nprecision_recall_fscore_support : 136 B\naccuracy_score : 136 B\nconfusion_matrix : 136 B\n_i2 : 755 B\nstopwords : 48 B\nWordNetLemmatizer : 1064 B\nword_tokenize : 136 B\ntrain_test_split : 136 B\nclassification_report : 136 B\n_exit_code : 16 B\nen_core_web_md : 72 B\ntext_to_nlp : 48 B\n_i3 : 621 B\ndatetime : 72 B\nparser : 72 B\n_i4 : 724 B\n_i5 : 120 B\n_i6 : 756 B\n_i7 : 621 B\ndate : 136 B\nlf : 28 B\nlt : 28 B\n_i8 : 724 B\nx_word2vec : 359280 B\ni : 28 B\ntext : 1387 B\n_i9 : 120 B\n_i10 : 214 B\n_i11 : 218 B\nl : 28 B\nr : 28 B\nm : 28 B\n_i12 : 218 B\n_i13 : 218 B\n_i14 : 214 B\n_i15 : 218 B\n_i16 : 724 B\n_i17 : 120 B\n_i18 : 718 B\n_i19 : 120 B\n_i20 : 955 B\n_i21 : 1009 B\n_i22 : 134 B\n_i23 : 146 B\n_i24 : 146 B\n_i25 : 174 B\n_i26 : 134 B\n_i27 : 146 B\n_i28 : 242 B\n_i29 : 243 B\n_i30 : 243 B\n_i31 : 243 B\n_i32 : 241 B\npickle : 72 B\nf : 168 B\n_i33 : 154 B\n_i34 : 153 B\nzarr : 72 B\n_i35 : 165 B\n_i36 : 165 B\n_i37 : 213 B\nnumcodecs : 72 B\n_i38 : 226 B\n_i39 : 239 B\n_i40 : 172 B\n_i41 : 245 B\n_i42 : 247 B\n_i43 : 250 B\n_i44 : 248 B\n_i45 : 257 B\n_i46 : 257 B\n_i47 : 245 B\n_i48 : 146 B\n_i49 : 158 B\n_i50 : 139 B\n_i51 : 134 B\n_i52 : 146 B\n_i53 : 218 B\n_i54 : 134 B\n_i55 : 155 B\n_i56 : 156 B\n_i57 : 218 B\n_i58 : 85 B\n_i59 : 85 B\n_i60 : 93 B\ntf : 72 B\n_i61 : 101 B\nkeras : 72 B\n_i62 : 1792 B\n_i63 : 1851 B\nragged_concat : 136 B\nMultiKernel1D : 896 B\nNewsNet : 896 B\n_i64 : 2569 B\n_i65 : 637 B\nfake_data : 69875829 B\ntrue_data : 86833062 B\n_i66 : 635 B\ndataset : 153842558 B\n_i67 : 2569 B\n_i68 : 186 B\n_i69 : 189 B\n_i70 : 2571 B\n_i71 : 189 B\nnewsNet : 48 B\noptimizer : 48 B\n_i72 : 756 B\nSTOP_WORDS : 32984 B\nWordCloud : 1064 B\nCountVectorizer : 1064 B\nLogisticRegression : 1064 B\n_i73 : 87 B\n_i74 : 140 B\n_i75 : 145 B\n_i76 : 121 B\n_i77 : 152 B\nDocBin : 1192 B\n_i78 : 157 B\n_i79 : 152 B\n_i80 : 270 B\nx_train : 269480 B\nx_test : 89896 B\ny_train : 538784 B\ny_test : 179616 B\n_i81 : 102 B\n_i82 : 722 B\n_i83 : 146 B\n"]}],"source":["# Memory mapper\n","print(*[f'{k} : {sys.getsizeof(v)} B' for (k, v) in globals().items()], sep='\\n')"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["np.save(os.path.join(\"E:\\\\Desktop\\\\Dataset\", 'word_vecs1'), x_word2vec[len(x_word2vec)//2:]) # Necessary!!!"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["44896\n"]}],"source":["# Search for first zero\n","l = 0\n","r = len(x_word2vec)-1\n","m = 0\n","while l < r:\n","    m = (l + m + 1)//2\n","    if x_word2vec[m] == 0:\n","        r = m\n","    else:\n","        l = m+1\n","print(m)"]},{"source":["# Load ML libraries\n","\n"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras"]},{"source":["\n","def ragged_concat(arr, axis=1):\n","    return tf.RaggedTensor.from_row_lengths(tf.concat(arr, axis=axis), row_lengths=tuple(len(i) for i in arr))\n","\n","conv3 = keras.layers.Conv1D(8, kernel_size=3, padding='same')\n","conv2 = keras.layers.Conv1D(4, kernel_size=2, padding='same')\n","conv5 = keras.layers.Conv1D(4, kernel_size=4, padding='same')\n"],"cell_type":"markdown","metadata":{}},{"source":["l = [conv5(tf.zeros((1, 12, 300))), conv3(tf.zeros((1, 12, 300))), conv2(tf.zeros((1, 12, 300)))]\n","# Shape is (Batch, Word Vecs, Word Vec size)\n","# 'same' padding fixes the word vec size\n","# We have different filter sizes\n","# (Last one)\n","tf.concat(l, 2)"],"cell_type":"markdown","metadata":{}},{"source":["## Build the model"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":223,"metadata":{},"outputs":[],"source":["\r\n","class MultiKernel1D(keras.layers.Layer):\r\n","    \"\"\"Supports using multiple kernel sizes for convoltion on text\"\"\"\r\n","    def __init__(self, filters:tuple=(100, 100, 100), kernels:tuple=(3,4,5), input_shape=(None, 300)):\r\n","        super(MultiKernel1D, self).__init__()\r\n","        self.kernels = [keras.layers.Conv1D(filter, kernel, input_shape=input_shape, padding='same') for (filter, kernel) in zip(filters, kernels)]\r\n","\r\n","    def call(self, inputs, training=None):\r\n","        # Call each of the kernels\r\n","        # Output should be like\r\n","        # Input shape is a tensor of size (A, 300)\r\n","        # Which means a variable vector of 300-vectors\r\n","        # Which represent word embeddings\r\n","\r\n","        # Each convolution outputs a vector that is (A, filters)\r\n","        # We need to combine them into a ragged tensor\r\n","        # [\r\n","        #   C1: [F1: [?, ?, ...], F2: [?, ?, ...], ...]\r\n","        #   C2: [F1: [?, ?, ...], F2: [?, ?, ...], ...]\r\n","        #   C3: [F1: [?, ?, ...], F2: [?, ?, ...], ...]\r\n","        # ]\r\n","        # 3 dimensions, ragged middle vector\r\n","        return tf.concat([kernel(inputs) for kernel in self.kernels], -1)\r\n","\r\n","        #     Conv 2\r\n","        #  /        /F1\r\n","        # Conv 1     F2\r\n","        # 0 | F1 F2 ... F3\r\n","        # 1 | F1 F2  F4 \r\n","        # 2 | F1 F2  /\r\n","        # 3 | F1 F2 /\r\n","        # 4 | F1 F2\r\n","        # So 3-d Tensor, or 4-d with batches\r\n","\r\n","\r\n","        \r\n","\r\n","class NewsNet(keras.Model):\r\n","    def __init__(self):\r\n","        super(NewsNet, self).__init__()\r\n","        self.kernels = MultiKernel1D()\r\n","\r\n","        self.sequential = keras.layers.LSTM(30)\r\n","        self.dense10 = keras.layers.Dense(10)\r\n","        self.predictor = keras.layers.Dense(1)\r\n","    \r\n","    def call(self, x, training=None):\n","\r\n","        x = self.kernels(x)\r\n","        x = self.sequential(x)\r\n","        x = self.dense10(x)\r\n","        return self.predictor(x)[0]\r\n"]},{"cell_type":"code","execution_count":224,"metadata":{},"outputs":[],"source":["newsNet = NewsNet()\n","optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n","x_train, x_test, y_train, y_test = train_test_split(x_word2vec, dataset['fake'].to_numpy(), train_size=0.07)\n","\n","loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)"]},{"cell_type":"markdown","metadata":{"id":"MA0H-ezT3A3-"},"source":["\n","# Training the model\n","\n"]},{"cell_type":"code","execution_count":225,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/3142 [00:00<?, ?it/s]\n","Start of epoch 0\n","Batch 0\n","(1, 231, 300)\n","Tensor(\"news_net_42/multi_kernel1d_40/concat:0\", shape=(1, 231, 300), dtype=float32)\n","(1, 231, 300)\n","Tensor(\"news_net_42/multi_kernel1d_40/concat:0\", shape=(1, 231, 300), dtype=float32)\n","  0%|          | 1/3142 [00:05<5:04:08,  5.81s/it]Training loss (for one batch) at step 0: 0.7678\n","Batch 1\n","(1, 371, 300)\n","Tensor(\"news_net_42/multi_kernel1d_40/concat:0\", shape=(1, 371, 300), dtype=float32)\n","  0%|          | 2/3142 [00:07<3:11:34,  3.66s/it]Batch 2\n","(1, 225, 300)\n","Tensor(\"news_net_42/multi_kernel1d_40/concat:0\", shape=(1, 225, 300), dtype=float32)\n","  0%|          | 3/3142 [00:10<2:39:16,  3.04s/it]Batch 3\n","(1, 529, 300)\n","Tensor(\"news_net_42/multi_kernel1d_40/concat:0\", shape=(1, 529, 300), dtype=float32)\n","  0%|          | 3/3142 [00:13<3:47:21,  4.35s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-225-249953ea86f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mx_batch_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0my_batch_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# Log every 200 batches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\lolzc\\Desktop\\AICamp\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\lolzc\\Desktop\\AICamp\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\lolzc\\Desktop\\AICamp\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\lolzc\\Desktop\\AICamp\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[1;32mc:\\Users\\lolzc\\Desktop\\AICamp\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\lolzc\\Desktop\\AICamp\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["## Note: Rerunning this cell uses the same model variables\r\n","global printed\r\n","model = newsNet\r\n","printed = False\r\n","def print_remove(s, *a, **k):\r\n","    global printed\r\n","    if (printed):\r\n","        print('\\r' + s, *a, **k)\r\n","    else:\r\n","        print(s, *a, **k)\r\n","\r\n","@tf.function\r\n","def train_step(x, y):\r\n","    with tf.GradientTape() as tape:\r\n","        logits = model(x, training=True)\r\n","        loss_value = loss_fn(y, logits)\r\n","    grads = tape.gradient(loss_value, model.trainable_weights)\r\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n","    train_acc_metric.update_state(y, logits)\r\n","    return loss_value\r\n","\r\n","\r\n","train_acc_metric = keras.metrics.BinaryAccuracy()\r\n","val_acc_metric = keras.metrics.BinaryAccuracy()\r\n","@tf.function\r\n","def test_step(x, y):\r\n","    val_logits = model(x, training=False)\r\n","    val_acc_metric.update_state(y, val_logits)\r\n","\r\n","\r\n","# Keep results for plotting\r\n","train_loss_results = []\r\n","train_accuracy_results = []\r\n","\r\n","import time\r\n","\r\n","epochs = 20\r\n","for epoch in range(epochs):\r\n","    print(\"\\nStart of epoch %d\" % (epoch,))\r\n","    start_time = time.time()\r\n","\r\n","    # Iterate over the batches of the dataset.\r\n","    r = tqdm.tqdm(range(len(x_train)))\r\n","    for step in r:\r\n","        printed = False\r\n","        r.write(f\"Batch {step}\")\r\n","        x_batch_train = tf.constant([[x.vector for x in x_train[step]]])\r\n","        y_batch_train = [y_train[step]]\r\n","        loss_value = train_step(x_batch_train, y_batch_train)\r\n","\r\n","        # Log every 200 batches.\r\n","        if step % 200 == 0:\r\n","            print_remove(\r\n","                \"Training loss (for one batch) at step %d: %.4f\"\r\n","                % (step, float(loss_value))\r\n","            )\r\n","\r\n","    # Display metrics at the end of each epoch.\r\n","    train_acc = train_acc_metric.result()\r\n","    print_remove(\"Training acc over epoch: %.4f\" % (float(train_acc),))\r\n","\r\n","    # Reset training metrics at the end of each epoch\r\n","    train_acc_metric.reset_states()\r\n","\r\n","    # Run a validation loop at the end of each epoch.\r\n","    for step in tqdm.tqdm(range(len(x_word2vec))):\r\n","        x_batch_val = tf.constant([[x.vector for x in x_test[step]]])\r\n","        y_batch_val = np.reshape(list(y_test[step]), shape=(1, 1))\r\n","        test_step(x_batch_val, y_batch_val)\r\n","\r\n","    val_acc = val_acc_metric.result()\r\n","    val_acc_metric.reset_states()\r\n","    print_remove(\"Validation acc: %.4f\" % (float(val_acc),))\r\n","    print_remove(\"Time taken: %.2fs\" % (time.time() - start_time))\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"WchEBxFeA46-"},"source":["## Exercise 4 \n","\n"]},{"cell_type":"code","metadata":{"id":"xv_PHkE6346w"},"source":["train_y_pred = baseline_model.predict(train_X)\n","print('Train accuracy', accuracy_score(train_y, train_y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ylD5OWTZoCz2"},"source":["We can see that we are not doing very well, but we are doing better than 50%. We can do the same for the val data to see how we are doing on unseen data, which is more valuable for us if we want to make predictions on new websites. Fill in the code below to evaluate val accuracy!"]},{"cell_type":"markdown","metadata":{"id":"Hq6sGGVHobjo"},"source":["We appear to be doing a bit worse on the val data, not much better than chance. To better understand the performance of our binary classification model, we should seek to better understand the mistakes that it is making. Specifically, when our model makes a mistake (about 40% of the time), are these mistakes false negatives or false positives?"]},{"cell_type":"markdown","metadata":{"id":"0A_HLeiz9IlH"},"source":["## Confusion Matrix "]},{"cell_type":"markdown","metadata":{"id":"VeWbCvMa8rhg"},"source":["To answer these questions, we produce and analyze the confusion matrix. The confusion matrix is a matrix that shows the following:\n","\n","![Confusion Matrix](https://cdn-images-1.medium.com/max/1600/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n","\n","where the terms mean\n","\n","* TP (True Positive) = You predicted positive (fake in our case, since fake has a label of 1) and it’s true.\n","* FP (False Positive) = You predicted positive and it’s false.\n","* FN (False Negative) = You predicted negative and it’s false.\n","* TN (True Negative) = You predicted negative and it’s true.\n","\n","\n","###Common Metrics\n","\n","From the confusion matrix, we can extract commonly used metrics like precision (TP/(TP + FP)) and recall (TP/(TP + FN)). \n","\n","* Precision quantifies how often the things we classify as positive are actually positive. For our task, this measures what fraction of the sites we classify as fake are actually fake. \n","* Recall quantifies what fraction of actually positive examples we classify as positive. In our case, this is the fraction of fake news websites that we actually identify as fake.\n","\n","Finally, a useful score to summarize both precision and recall is the F-1 score. This is just a simple function (the harmonic mean) of precision and recall, shown in the summary below:\n","\n","<img src=\"https://datascience103579984.files.wordpress.com/2019/04/capture3-24.png\" width=\"400\" height=\"200\"></img>"]},{"cell_type":"markdown","metadata":{"id":"pftTwsCvE8Lt"},"source":["##Exercise 5 |  Using the Confusion Matrix "]},{"cell_type":"markdown","metadata":{"id":"tc9BxsQ-GYOo"},"source":["Run the cell below to create the confusion matrix for our own model. "]},{"cell_type":"code","metadata":{"id":"M9aYm4oqsL6P"},"source":["print('Confusion matrix:')\n","print(confusion_matrix(val_y, val_y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q5Of0ZHsHZ0C"},"source":["A Confusion Matrix can quickly tell you how well your model is doing. The primary way to figure this out is to calculate the Error Rate. \n","\n","The Error Rate is:   (FP) + (FN)) / (TP + FP + FN + TN).\n","\n","This is just all the false predictions (False Negative + False Positive) divided by all the predictions added together.  \n","\n","Use the Confusion Matrix we just created to calculate the Error Rate for our model. "]},{"cell_type":"code","metadata":{"id":"Bo0Bej9z_xcq"},"source":["### YOUR CODE HERE ###\n","\n","### END CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ac9XXM9XsU0"},"source":["## Exercise 6"]},{"cell_type":"code","metadata":{"id":"gVxJhwhzspF-"},"source":["print(val_y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nh-qi0tvGvFH"},"source":["We can see that we have many false negatives, and not as many false positives. Why is this the case? If we print out *val_y_pred*, we can see that our model is mostly predicting 0's (websites are real)."]},{"cell_type":"markdown","metadata":{"id":"fa5GCh7eG7OU"},"source":["What fraction of predictions in *val_y_pred* are 1's? Hint: you may find *np.mean* useful."]},{"cell_type":"code","metadata":{"id":"pStoI4cLfKDv"},"source":["### YOUR CODE HERE ###\n","\n","### END CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQdhOeigtmCp"},"source":["Why so many 0's? The only information we are giving our model is its domain name extension. It's natural that the model would learn that websites with \".biz\" extensions are unlikely to be reliable news websites, but it is still the case that most websites in the dataset (fake and real) have \".com\" extensions. Thus, our model will misclassify many fake news websites with \".com\" extensions as real. "]},{"cell_type":"code","metadata":{"id":"tHQTv6L98uno"},"source":["prf = precision_recall_fscore_support(val_y, val_y_pred)\n","\n","print('Precision:', prf[0][1])\n","print('Recall:', prf[1][1])\n","print('F-Score:', prf[2][1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vw9xDPTsruaB"},"source":["Again, the precision and recall metrics suggest that when we classify a website as fake, we are usually right, but we are not doing great at classifying these websites as fake frequently enough."]},{"cell_type":"markdown","metadata":{"id":"vyJC74B7ALAy"},"source":["##Using Keywords for a Stronger Baseline "]},{"cell_type":"markdown","metadata":{"id":"_3lY7pMkAPpX"},"source":["The key problem with our model in its current state is that it simply does not have enough information. This should not be a surprise–it was pretty unlikely in the first place that domain name extensions would be enough. If you like, feel free to add a few more extensions in the “featurizer” above and re-run all the code for evaluation–you'll find it doesn't make much of a difference.\n","Where can we get more information about webpages? From the HTML! Remember that the HTML contains all of the text and structure of a webpage. If we cleverly choose features from the HTML to feed into our logistic regression model, we will drastically improve our performance. We saw yesterday that probing hypotheses related to the counts of hypotheses words produced interesting results, and we will continue in this direction today to produce a model that leverages these differences in word frequencies.\n"]},{"cell_type":"markdown","metadata":{"id":"TyRp6ooJAnng"},"source":["## Exercise 7: Instructor-Led Discussion on Better Input Features\n"]},{"cell_type":"markdown","metadata":{"id":"L4jjZ5HF35K0"},"source":["\n","The below code introduces a better featurizer that counts the number of keywords (normalized using the *log* function) in the HTML. Normalizing the counts is a trick that prevents the featurized values from becoming too extreme. Read the code and make sure you understand what it is doing. Then add \"sports\" and \"finance\" as additional keywords to expand our model.\n","\n","**Run the below code and discuss what it is doing as a class. Add in additional keywords to further expand our model as you see fit.**\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sk_BBflEAuX3"},"source":["##Exercise 8"]},{"cell_type":"markdown","metadata":{"id":"bgWx3s7UxDue"},"source":["\n","\n","Let's run and evaluate the above featurizer. Add in code to fit the model, compute train accuracy, val accuracy, val confusion matrix, and val precision, recall, and F1-Score, just as before."]},{"cell_type":"code","metadata":{"id":"fxEIPqn1f1iQ"},"source":["train_X, train_y, feature_descriptions = prepare_data(train_data, keyword_featurizer)\n","val_X, val_y, feature_descriptions = prepare_data(val_data, keyword_featurizer)\n","\n","print('Number of features per example:', len(train_X[0]))\n","print('Feature descriptions:')\n","print(feature_descriptions)\n","print()\n","  \n","baseline_model = LogisticRegression()\n","\n","### YOUR CODE HERE ###\n","\n","### END CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1l9Jq6CzwWOt"},"source":["## Interpreting our Model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_hTNITBgB871"},"source":["### Instructor-Led Discussion: Interpreting Input Variables"]},{"cell_type":"markdown","metadata":{"id":"43z3A67xB4Qu"},"source":["As mentioned earlier, a key motivation for using a simpler model is interpretability.\n","\n","We've learned that the prediction of a logistic regression classifier is just the output of a multiplication with model weights, followed by a non-linear transformation (sigmoid). Because the sigmoid function is always increasing (monotonic) on its domain (see below), we know that if the dot product (or multiplication of vectors) between model weights and input features is large, then the output prediction will be closer to 1. If the dot product is small, then the output prediction will be closer to 0.\n","\n","![Sigmoid](https://cdn-images-1.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png)\n","\n","Thus, the weights corresponding to features tell us whether the features are important in the classification. If the weight corresponding to the feature \".net domain\" has a large positive value, then websites with \".net\" domains are more likely to be classified as fake (since fake has label 1). If it has a large negative value, then these websites are more likely to be classified as real. If it has value close to 0, then the feature may not be useful (at least, it may not be useful given that the other features are present).\n"]},{"cell_type":"markdown","metadata":{"id":"nqIKnF6fCgnP"},"source":["###Using Feature Descriptions"]},{"cell_type":"markdown","metadata":{"id":"0G8LwtG2Cj9u"},"source":["Let's see what weights our model learned. The code below uses *feature_descriptions* and the weights, or coefficients, of the model and sorts them in ascending order."]},{"cell_type":"code","metadata":{"id":"QHcJkqd4zic8"},"source":["sorted(zip(feature_descriptions, baseline_model.coef_[0].tolist()), key=lambda x: x[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qfIAQeg5zpRL"},"source":["## Exercise 9\n","\n","Answer the following questions:\n","\n","* What features have positive weight (most predictive of being fake)? What does that indicate?\n","* Which ones have negative weight (most predictive of being real)? What does that indicate?\n","* Which ones have close to 0 weight? \n","* Are there any feature weights that surprise you? \n","* Try coming up with explanations for why the feature weights are the way they are. Does this help you come up with new feature ideas? (~15 minutes)"]},{"cell_type":"code","metadata":{"id":"gnYk7Vp_YiSO"},"source":["'''\n","- \n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"082Dut9agst6"},"source":["## Instructor-Led Discussion: Final Interpretation of Inputs"]},{"cell_type":"markdown","metadata":{"id":"gfQvDlNAYjAa"},"source":["##Exercise 10 |  Final Baseline\n","\n","Finally, play around with the last few cells, adding more keywords and domain names to see how the results change. Note that \"keywords\" can be a variety of things: English words, English phrases (spaces are allowed), HTML tags, and any other string present in HTML. Also notice how the weights on different features vary–you may observe some interesting effects. When you are done, run the cell below to run evaluations again!"]},{"cell_type":"code","metadata":{"id":"19rbiCbuP8iq"},"source":["train_y_pred = baseline_model.predict(train_X)\n","print('Train accuracy', accuracy_score(train_y, train_y_pred))\n","\n","val_y_pred = baseline_model.predict(val_X)\n","print('Val accuracy', accuracy_score(val_y, val_y_pred))\n","\n","print('Confusion matrix:')\n","print(confusion_matrix(val_y, val_y_pred))\n","\n","prf = precision_recall_fscore_support(val_y, val_y_pred)\n","\n","print('Precision:', prf[0][1])\n","print('Recall:', prf[1][1])\n","print('F-Score:', prf[2][1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6aOTjSX0_yj"},"source":["Congratulations on completing this notebook. Looking at the results of our final baseline, you may be surprised this approach is working at all–after all, our model is still barely looking at the content of websites. We will further explore the issue of modeling the content of websites tomorrow, but as a result of our efforts today, we now know that we can make progress with a relatively simple approach!"]}]}