{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"StudentCopy_Section_2_FakeNews.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3810jvsc74a57bd01367987cc7840cfe11a5e48493d0489ed8a2d67afc5da873c4c87b7776f6181c","display_name":"Python 3.8.10 64-bit ('venv': venv)"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gVgIxW-YPeQH"},"source":["# Fake News Classification\n","\n","\n","### Connect Colab to VSCode\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install kora -q\n","from kora import jupyter\n","jupyter.start(lab=True)\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"source":["## Import Libraries"],"cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qT8bCpXGr2nO"},"source":["import tqdm\n","#@title Run this code to get started\n","imports = {\n","    #'pandas':'pd',\n","    'numpy': 'np',\n","    'matplotlib.pyplot': 'plt',\n","    'sklearn':None,\n","    'string':None,\n","    'nltk':None,\n","    'spacy':None,\n","    'wordcloud':None,\n","    'sys':None,\n","    'os':None}\n","\n","print(\"Importing libraries\")\n","for name in tqdm.tqdm(imports):\n","    try:\n","        alias = imports[name] or name\n","        globals()[alias] = __import__(name)\n","    except ImportError:\n","        print(f\"Failed importing {name}\")\n","\n","# import math\n","# import os\n","# import numpy as np\n","# import pandas as pd\n","\n","# import pickle\n","\n","# import requests, io, zipfile\n","\n","# Download class resources...\n","\n","basepath = 'E:\\\\Desktop\\\\Dataset'\n","#basepath = 'drive/MyDrive/Notebooks/Fake_news_data'\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9/9 [00:00<?, ?it/s]Importing libraries\n","\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Importing NLP\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\lolzc\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\lolzc\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["print(\"Importing NLP\")\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from spacy.lang.en.stop_words import STOP_WORDS\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import accuracy_score\n","#!python -m spacy download en_core_web_md\n","import en_core_web_md\n","text_to_nlp = en_core_web_md.load()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"source":["# Dataset Views"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Topics:\\n\\t\", *list(set(dataset['subject'])), sep='\\n\\t')\n","\n","dataset.head()\n","\n","dataset['date'].min()"]},{"source":["### Generate Word cloud for fake/real news"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fake = True\n","print(\"Fake news Word cloud\")\n","news_text = ''\n","for t in dataset[dataset['fake'] == fake]['text'].values: # form field cell\n","    news_text += t + ' '\n","    \n","wordcloud = WordCloud()    \n","wordcloud.generate_from_text(news_text)\n","plt.figure(figsize=(14,7))\n","plt.imshow(wordcloud, interpolation='bilinear')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fake = False\n","print(\"Real news Word cloud\")\n","news_text = ''\n","for t in dataset[dataset['fake'] == fake]['text'].values: # form field cell\n","    news_text += t + ' '\n","    \n","wordcloud = WordCloud()    \n","wordcloud.generate_from_text(news_text)\n","plt.figure(figsize=(14,7))\n","plt.imshow(wordcloud, interpolation='bilinear')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fake = True\n","print(\"Fake news Title cloud\")\n","news_text = ''\n","for t in dataset[dataset['fake'] == fake]['title'].values: # form field cell\n","    news_text += t + ' '\n","    \n","wordcloud = WordCloud()    \n","wordcloud.generate_from_text(news_text)\n","plt.figure(figsize=(14,7))\n","plt.imshow(wordcloud, interpolation='bilinear')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fake = False\n","print(\"True news Title cloud\")\n","news_text = ''\n","for t in dataset[dataset['fake'] == fake]['title'].values: # form field cell\n","    news_text += t + ' '\n","    \n","wordcloud = WordCloud()    \n","wordcloud.generate_from_text(news_text)\n","plt.figure(figsize=(14,7))\n","plt.imshow(wordcloud, interpolation='bilinear')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n"]},{"source":["## Prepare data for NLP"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# SKIP\n","# if not \"text_to_nlp\" in globals():\n","#     print(\"Loading NLP converter\")\n","#     text_to_nlp = en_core_web_md.load() #Prepare Spacy\n","# print(\"Tokenizing text\")\n","\n","\n","# x_word2vec = np.zeros((len(dataset),), dtype=object)\n","# i = 0\n","# for text in tqdm.tqdm(dataset['text']):\n","#     x_word2vec[i] = text_to_nlp(text) # Array of tokens, passed to model\n","#     i += 1\n","\n","\n","# # print(\"Tokenizing titles\")\n","# # x_titlevec = np.zeros((len(dataset),), dtype=object)\n","# # i = 0\n","# # for text in tqdm.tqdm(dataset['title']):\n","# #     x_titlevec[i] = text_to_nlp(text) # Array of tokens, passed to model\n","# #     i += 1\n","# # x_titlevec = np.array(x_word2vec)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Memory mapper\n","print(*[f'{k} : {sys.getsizeof(v)} B' for (k, v) in globals().items()], sep='\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# x_word2vec = np.concat(np.load(os.path.join(\"E:\\\\Desktop\\\\Dataset\", 'word_vecs1')), np.load(os.path.join(\"E:\\\\Desktop\\\\Dataset\", 'word_vecs')) # Necessary!!!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Search for first zero\n","# l = 0\n","# r = len(x_word2vec)-1\n","# m = 0\n","# while l < r:\n","#     m = (l + m + 1)//2\n","#     if x_word2vec[m] == 0:\n","#         r = m\n","#     else:\n","#         l = m+1\n","# print(m)"]},{"source":["# Load ML libraries\n","\n"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"code","execution_count":231,"metadata":{},"outputs":[],"source":["classes = [b'fake', b'satire', b'bias', b'conspiracy', b'state', b'junksci', b'hate', b'clickbait', b'unreliable', b'political', b'reliable', b'unknown']\n","\n","# Generate onehot mapping\n","hot = {}\n","for idx, clss in enumerate(classes):\n","    hot[clss] = tf.reshape(tf.constant([0.] * max(idx, 0) + [1.] + [0.] * max(len(classes)-idx-1, 0), dtype=np.float32), shape=(1, 12))\n","\n","len_data = 9408908\n","test_size = 0.005\n","batch_size = 1\n","\n","test_len = int((len_data * test_size)//batch_size) * batch_size\n","train_len = len_data - test_len"]},{"cell_type":"code","execution_count":213,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{},"execution_count":213}],"source":["len(hot[b'bias'])"]},{"cell_type":"code","execution_count":220,"metadata":{},"outputs":[],"source":["dataset = tf.data.experimental.make_csv_dataset(os.path.join(basepath, 'news.csv'), batch_size=batch_size, select_columns=['content', 'type'], label_name='type', shuffle=True)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["for k in dataset:\n","    a = k\n","    break"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["from collections import OrderedDict\n","k: OrderedDict = k"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=string, numpy=b'bias'>"]},"metadata":{},"execution_count":83}],"source":[]},{"source":["\n","def ragged_concat(arr, axis=1):\n","    return tf.RaggedTensor.from_row_lengths(tf.concat(arr, axis=axis), row_lengths=tuple(len(i) for i in arr))\n","\n","conv3 = keras.layers.Conv1D(8, kernel_size=3, padding='same')\n","conv2 = keras.layers.Conv1D(4, kernel_size=2, padding='same')\n","conv5 = keras.layers.Conv1D(4, kernel_size=4, padding='same')\n"],"cell_type":"markdown","metadata":{}},{"source":["l = [conv5(tf.zeros((1, 12, 300))), conv3(tf.zeros((1, 12, 300))), conv2(tf.zeros((1, 12, 300)))]\n","# Shape is (Batch, Word Vecs, Word Vec size)\n","# 'same' padding fixes the word vec size\n","# We have different filter sizes\n","# (Last one)\n","tf.concat(l, 2)"],"cell_type":"markdown","metadata":{}},{"source":["## Build the model"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":221,"metadata":{},"outputs":[],"source":["\n","class MultiKernel1D(keras.layers.Layer):\n","    \"\"\"Supports using multiple kernel sizes for convoltion on text\"\"\"\n","    def __init__(self, filters:tuple=(180, 100), kernels:tuple=(3,5), input_shape=(None, 300)):\n","        super(MultiKernel1D, self).__init__()\n","        self.kernels = [keras.layers.Conv1D(filter, kernel, input_shape=input_shape, padding='same') for (filter, kernel) in zip(filters, kernels)]\n","\n","    def call(self, inputs, training=None):\n","        # Call each of the kernels\n","        # Output should be like\n","        # Input shape is a tensor of size (A, 300)\n","        # Which means a variable vector of 300-vectors\n","        # Which represent word embeddings\n","\n","        # Each convolution outputs a vector that is (A, filters)\n","        # We need to combine them into a ragged tensor\n","        # [\n","        #   C1: [F1: [?, ?, ...], F2: [?, ?, ...], ...]\n","        #   C2: [F1: [?, ?, ...], F2: [?, ?, ...], ...]\n","        #   C3: [F1: [?, ?, ...], F2: [?, ?, ...], ...]\n","        # ]\n","        # 3 dimensions, ragged middle vector\n","        return tf.concat([kernel(inputs) for kernel in self.kernels], -1)\n","\n","        #     Conv 2\n","        #  /        /F1\n","        # Conv 1     F2\n","        # 0 | F1 F2 ... F3\n","        # 1 | F1 F2  F4 \n","        # 2 | F1 F2  /\n","        # 3 | F1 F2 /\n","        # 4 | F1 F2\n","        # So 3-d Tensor, or 4-d with batches\n","\n","\n","        \n","\n","class NewsNet(keras.Model):\n","    def __init__(self):\n","        super(NewsNet, self).__init__()\n","        self.kernels = MultiKernel1D()\n","        self.sequential = keras.layers.LSTM(30)\n","        self.predictor = keras.layers.Dense(len(classes), activation=keras.activations.softmax) # [Real, Fake]\n","    \n","    def call(self, x, training=None):\n","\n","        x = self.kernels(x)\n","        x = self.sequential(x)\n","        return self.predictor(x)\n"]},{"cell_type":"code","execution_count":228,"metadata":{},"outputs":[],"source":["newsNet = NewsNet()\n","optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n","loss_fn = keras.losses.CategoricalCrossentropy()\n"]},{"cell_type":"markdown","metadata":{"id":"MA0H-ezT3A3-"},"source":["\n","# Training the model\n","\n"]},{"cell_type":"code","execution_count":235,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":["proved of the job Trump is doing, while just 38 percent approved of the president\\xe2\\x80\\x99s performance.\\n\\nHouse Minority Leader Nancy Pelosi, who led the party to a majority more than a decade ago, is watching the numbers closely.\\n\\n\\xe2\\x80\\x9cOh my gosh, he\\xe2\\x80\\x99s under 40,\\xe2\\x80\\x9d Pelosi said on Thursday, referring to Trump\\xe2\\x80\\x99s low approval rating.\\n\\n\\xe2\\x80\\x9cHistory is on our side,\\xe2\\x80\\x9d she added. \\xe2\\x80\\x9cWhen President Clinton\\'s numbers went down, they won. When President Bush\\'s numbers [went down], we won. When President Obama\\'s numbers went down, they won.\\xe2\\x80\\x9d\\n\\nAnalysts speculate the surge in retirements is a sign lawmakers are fearful that voter disapproval of Trump will drag down the ticket and cause major Republican losses down the ballot in November.\\n\\nThe head of the House GOP campaign arm disagreed. Rep. Steve Stivers, R-Ohio, who heads the National Republican Congressional Committee, said the retirements are not provoked by Trump, but rather the GOP term limit system for committee chairs.\\n\\nRoyce is about to conclude a six-year term as the top Republican on the House Foreign Affairs Committee, while Issa\\xe2\\x80\\x99s term as chairman of the House Oversight and Government Reform Committee ended two years ago.\\n\\nWhen the immediate prospects of chairing a new panel dims, many lawmakers retire.\\n\\n\\xe2\\x80\\x9cWhen their six years is up, they tend to leave, and that opens up some seats,\\xe2\\x80\\x9d Stivers said. \"We see that time and again.\"\\n\\nStivers said the party already has found \\xe2\\x80\\x9cgood recruits\\xe2\\x80\\x9d to run in both districts and pointed to the GOP\\xe2\\x80\\x99s victory in all five special election House races this year. The congressional district map also favors Republicans, Stivers said.\\n\\n\\xe2\\x80\\x9cWe clearly have to defy history,\\xe2\\x80\\x9d Stivers said. \\xe2\\x80\\x9cBut I feel comfortable we are going to be able to do it. We have great candidates, solid lines, and we\\xe2\\x80\\x99ve got the money that it takes to win.\\xe2\\x80\\x9d'],\n","      dtype=object)>)])\n","OrderedDict([('content', <tf.Tensor: shape=(1,), dtype=string, numpy=\n","array([b'Brexit: UK Leaves and Now 5 Countries Want To Follow, Wait Until You See What Happens Next!\\n\\n% of readers think this story is Fact. Add your two cents.\\n\\nHeadline: Bitcoin & Blockchain Searches Exceed Trump! Blockchain Stocks Are Next!\\n\\nBy Lisa Haven\\n\\nTrouble is brewing in Europe where the United Kingdom has officially left the EU, causing global economic panic amoung the globalists. And now five other countries may follow suit. So what comes next and how worried should we be?\\n\\nIn the video below The Money GPS reveals his thoughts behind the Brexit\\xe2\\x80\\xa6\\n\\nPersonally, I believe the country voted to exit because they chose FREEDOM over Bondage! It\\xe2\\x80\\x99s nothing less than a populist revolt and a full rejection of the political establishment!! AMEN!\\n\\nBut remember the establishment will fight, with full force, against anyone, or any country, who stands in their way of a global governance. So you can guarantee this exit wont come without turmoil.\\n\\nMore Critical Reads You Need to Hear by Lisa Haven! Click Here!\\n\\nSubscribe to My Website at: www.FreedomNationNews.com\\n\\nCheck Me out On Youtube'],\n","      dtype=object)>)])\n","OrderedDict([('content', <tf.Tensor: shape=(1,), dtype=string, numpy=\n","array([b'Spread the LOVE!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nby Gregg Prescott, M.S.\\n\\nEditor, In5D.com\\n\\nFor this meditation, wear headphones for optimal results! The Schumann Resonance is a frequency of sound generated between the symbiotic relationship of our planet and the cosmos. Many regard this frequency as rising in nature and benevolent to spiritual meditations as well as physical healing.\\n\\nThe audio on this meditation video will bring you from the traditional Schumann\\xe2\\x80\\x99s resonance all the way down to a zero hertz frequency in the low delta epsilon range then slowly moves back upwards to a resonance similar to that of an Out Of Body Experience resonance.\\n\\nThe base frequency was adjusted to resonate with the 432Hz natural harmonics and to align healing, consciousness and spiritual expansion within your DNA code. Additional holophonic sounds and theta binaural beats we also layered in to maximize your listening and meditation experience!\\n\\nHolophonic, binaural theta wave meditations help to facilitate the development of the 3rd eye, which innately understands the symbols and pictures of multidimensional stimuli.\\n\\nWear headphones for optimal results!\\n\\nThe original file can be found here\\n\\nAlso see:\\n\\n528 Hz \\xe2\\x80\\x93 Love frequency \\xe2\\x80\\x9cDNA integrity and repair\\xe2\\x80\\x9d\\n\\nBe sure you protect yourself before any meditation!\\n\\nMeditation Techniques: What to do BEFORE You Meditate\\n\\nFollow In5D on Facebook!\\n\\nClick here for more articles by Gregg Prescott!\\n\\nAbout the Author:\\n\\nGregg Prescott, M.S. is the founder and editor of In5D and BodyMindSoulSpirit. You can find his In5D Radio shows on the In5D Youtube channel. Gregg is also a transformational speaker and promotes spiritual, metaphysical and esoteric conferences in the United States through In5dEvents. His love and faith for humanity motivates him to work in humanity\\xe2\\x80\\x99s best interests 12-15+ hours a day, 365 days a year. Please like and follow In5D on Facebook, BodyMindSoulSpirit on Facebook and In5D on YouYube!\\n\\nUse Facebook to Comment on this Post'],\n","      dtype=object)>)])\n","OrderedDict([('content', <tf.Tensor: shape=(1,), dtype=string, numpy=\n","array([b'Hidden among the dense rainforests of the Amazon exists a dwindling number of indigenous peoples whose isolated cultures remain untouched by the industrialized world. And in one of the most striking examples of just how cut off many of these civilizations are, scientists have come into contact with a remote Amazon tribe that is only on season two of The Sopranos.\\n\\nWow! What an amazing discovery!\\n\\nOn a recent Amazonian expedition, a team of archaeologists reportedly encountered a previously undiscovered tribe that has only finished watching the 13th episode of the show\\xe2\\x80\\x99s second season, \\xe2\\x80\\x9cFunhouse,\\xe2\\x80\\x9d in which Tony is arrested by the FBI. According to their research, the isolated tribesmen and women have extensive knowledge about Tony Soprano\\xe2\\x80\\x99s family and criminal history from early plotlines, but have no idea where that will take him in the third through seventh seasons.\\n\\n\\xe2\\x80\\x9cWhile the tribe knew about the inner workings of the DiMeo family\\xe2\\x80\\x99s tenuous relationship with the Mafia, these isolated peoples have yet to see the aftermath of Big Pussy Bonpensiero ratting them out to the FBI,\\xe2\\x80\\x9d said Robert Abreu, emphasizing that this 30-person group is far behind other cultures in its knowledge of the HBO classic. \\xe2\\x80\\x9cRight now, they are only invested in Tony; his mother; Jennifer Melfi; and his prot\\xc3\\xa9g\\xc3\\xa9, Christopher. If any of these people were to see Tony Soprano\\xe2\\x80\\x99s cousin, Tony Blundetto, or other characters from beyond the show\\xe2\\x80\\x99s 26th episode, they would be both confused and shocked, to say the least. \\xe2\\x80\\x9d\\n\\nAfter weeks of interviewing the remote people, researchers further admitted they were stunned at how in-depth the tribe\\xe2\\x80\\x99s knowledge of the first two seasons was, and theorize that its isolation had allowed its opinions to not be clouded by the remaining seven seasons. However, their findings also noted that the tribe still believed that Christopher would still get back with his girlfriend, Adriana, and had no idea that the show would take a nosedive during a critically panned season six or end in such a controversial season seven finale.\\n\\nTruly a stunning find!\\n\\nWhile there\\xe2\\x80\\x99s no telling how this tribe will react when season three begins to creep into members\\xe2\\x80\\x99 lives, it will definitely be interesting to see how they react. Until then, let\\xe2\\x80\\x99s hope that the researchers can avoid spoilers and let the tribe discover seasons three through seven of The Sopranos on its own!'],\n","      dtype=object)>)])\n","OrderedDict([('content', <tf.Tensor: shape=(1,), dtype=string, numpy=\n","array([b'CLANCY OVERELL | Editor | Contact\\n\\nFormer Prime Minister Tony Abbott has today requested that the media please do not undermine his politics by asking his youngest daughter, Frances, what she thinks about having the national holiday on January 26th.\\n\\nIn a throwback to the gay marriage postal vote, where Abbott\\xe2\\x80\\x99s own daughter campaigned against his wishes to never allow gay people the right to get married \\xe2\\x80\\x93 Abbott has requested that the media just stop asking her for her compassionate millennial female opinions on issues that could be quite simply resolved with a slight gesture.\\n\\nToday\\xe2\\x80\\x99s comments come as the former chaplaincy student appeared on 2GB talkback this morning to argue that the events of January 26, 1788, were a \\xe2\\x80\\x9cgood thing\\xe2\\x80\\x9d for Aboriginal people.'],\n","      dtype=object)>)])\n","OrderedDict([('content', <tf.Tensor: shape=(1,), dtype=string, numpy=\n","array([b'1282\\n\\nBy The Pete Santilli Show on Thursday Dec 07 2017 12:56'],\n","      dtype=object)>)])\n","OrderedDict([('content', <tf.Tensor: shape=(1,), dtype=string, numpy=\n","array([b'Today Warren Cole Smith Interviews Dr. Bill Brown, the National Director of the Colson Fellows Program.\\n\\nWe\\xe2\\x80\\x99re re-airing this podcast because we are pleased to announce that the Colson Center is now accepting applications for the 2018-2019 class of Colson Fellows! The Colson Fellows Program equips people of strong faith with relevant skills, biblical knowledge, and a solid Christian worldview to navigate today\\xe2\\x80\\x99s culture and play a significant role in helping to restore it in a way that is pleasing to God.'],\n","      dtype=object)>)])\n","OrderedDict([('content', <tf.Tensor: shape=(1,), dtype=string, numpy=\n","array([b'FBI Chief FOIA Officer: \\xe2\\x80\\x9cEvery Single Memo Comey Leaked Was Classified\\xe2\\x80\\x9d\\n\\n% of readers think this story is Fact. Add your two cents.\\n\\nHeadline: Bitcoin & Blockchain Searches Exceed Trump! Blockchain Stocks Are Next!\\n\\nAs James Comey trots around the country on his book-selling tour, tweeting bible quotes and nature scenes, a less serene series of events is playing out in Washington D.C. which suggests the former FBI Director may be in trouble over the memos he leaked to Columbia law professor Daniel Richman, allegedly detailing conversations between Comey and President Trump.\\n\\nJames Comey, Daniel Richman\\n\\nWhile Richman told CNN \\xe2\\x80\\x9cNo memo was given to me that was marked \\xe2\\x80\\x98classified,\\xe2\\x80\\x99 and James Comey told Congressional investigators he tried to \\xe2\\x80\\x9cwrite it in such a way that I don\\xe2\\x80\\x99t include anything that would trigger a classification,\\xe2\\x80\\x9d it appears the FBI\\xe2\\x80\\x99s chief FOIA officer disagrees.\\n\\nWhile we previously reported that Senator Chuck Grassley (R-IA) said four of the 7 Comey memos he reviewed were \\xe2\\x80\\x9cmarked classified\\xe2\\x80\\x9d at the \\xe2\\x80\\x9cSecret\\xe2\\x80\\x9d or \\xe2\\x80\\x9cConfidential\\xe2\\x80\\x9d level \\xe2\\x80\\x93 tonight we find out that every single Comey memo was classified at the time, per Judicial Watch director of investigations Chris Farrell \\xe2\\x80\\x93 who has a signed declaration from the FBI\\xe2\\x80\\x99s chief FOIA officer to that effect:\\n\\nWe have a sworn declaration from David Hardy who is the chief FOIA officer of the FBI that we obtained just in the last few days, and in that sworn declaration, Mr. Hardy says that all of Comey\\xe2\\x80\\x99s memos \\xe2\\x80\\x93 all of them, were classified at the time they were written, and they remain classified. \\xe2\\x80\\x93Chris Farrell, Judicial Watch\\n\\nTherefore, Farrell points out, Comey mishandled national defense information when he \\xe2\\x80\\x9cknowingly and willfully\\xe2\\x80\\x9d leaked them to his friend at Columbia University.\\n\\nIt\\xe2\\x80\\x99s also mishandling of national defense information, which is a crime. So it\\xe2\\x80\\x99s clear that Mr. Comey not only authored those documents, but then knowingly and willfully leaked them to persons unauthorized, which is in and of itself a national security crime. Mr. Comey should have been read his rights back on June 8th when he testified before the Senate.\\n\\nIn closing, Farrell tells Dobbs \\xe2\\x80\\x9cRecently retired and active duty FBI agents have told me \\xe2\\x80\\x93 and it\\xe2\\x80\\x99s several of them, they consider Comey to be a dirty cop.\\xe2\\x80\\x9d\\n\\nHeres hoping 2018 brings more ethical leadership, focused on the truth and lasting values. Happy New Year, everybody. James Comey (@Comey) December 31, 2017\\n\\nWhat was that about \\xe2\\x80\\x9cmore ethical leadership\\xe2\\x80\\x9d Jim?\\n\\nSource: http://silveristhenew.com/2018/01/05/fbi-chief-foia-officer-every-single-memo-comey-leaked-was-classified/'],\n","      dtype=object)>)])\n","OrderedDict([('content', <tf.Tensor: shape=(1,), dtype=string, numpy=\n","array([b'The ongoing debate regarding the legalization of cannabis is one that has captivated the nation, with both supporters and those against it being made openly available passionately arguing their indivi\\xe2\\x80\\xa6'],\n","      dtype=object)>)])\n","OrderedDict([('content', <tf.Tensor: shape=(1,), dtype=string, numpy=\n","array([b'In a move sending waves of shock and awe through the entire armed services, G.I. Joe, with Lifelike Beard, was honored with his first Medal of Honor for service above and beyond the call of duty. This news comes on the heels of the surprise announcement that President Barack Obama had been chosen for the 2009 Nobel Peace Prize.\\n\\nBarack was informed of his win over his daily bowl of Lucky Charms, while still in his WH pajamas, and he immediately leapt up to dance a jig. This achievement occurred despite the fact he was unaware he was on the Nobel short list, or that he even was technically eligible for the award after 11 days of unfettered yeoman\\xe2\\x80\\x99s service.\\n\\nAsked about Joe\\xe2\\x80\\x99s selection, the POTUS answered with his typical humble aplomb, \\xe2\\x80\\x9cI know the excitement and joy both he and his family are feeling right now. He\\xe2\\x80\\x99s probably doing just like I am\\xe2\\x80\\x94pinching himself while asking, \\xe2\\x80\\x9cIs this Real?!!\\xe2\\x80\\x9d\\n\\nGI Joe came into this world in 1964 and is one of the few soldiers to go on active duty almost immediately afterwards. His list of exploits and brave maneuvers are certainly too numerous to mention. But his victories in various engagements, such as the Skirmish at Malibu Ridge against Ken and Klaus Barbie; Gunfight at Popsicklestick Coral versus Kid Cabbage Patch; His Marine Battles with Sponge Bob; and of course his gory Cage Fight with Mr. Potatohead in which his opponent\\xe2\\x80\\x99s ear and nose were severed, and eyeball gouged out, have all gone down in the annals of America\\xe2\\x80\\x99s most heroic fights. Joe was able to endure great pain without complaint, and even survive amazing injuries without treatment. When asked, Joe attributed much of his success to his Kung-Fu Grip.\\n\\nThe comparisons with Obama are obvious, and every bit as real. What did Obama do to cinch the award in his first eleven days? Even before his reign began, he prophesied his mere election would bring down down swollen rivers and rising tides from Global Warming. Of course this has already started to occur. He then passed the Stimulus Bill which singlehandedly solved the world\\xe2\\x80\\x99s financial crisis, which would have made the Great Depression look like a failed after school bake sale. He has solved the healthcare problems of all Americans for future generations with a few speeches (excepting a few minor details still to be addressed).\\n\\nObama has taught both GM and Chrysler the meaning of work ethic, salesmanship and capitalism. He has solved the race divide with a chastisement of stupidly out-of-control cops, a further pep talk and a humble beer. Obama has also closed Guantanamo Bay (but for for a few bureaucratic details). In doing so he has instructed the world that even if you are accused of terrorism, it doesn\\xe2\\x80\\x99t mean you can\\xe2\\x80\\x99t get a breakfast every good as one offered at the Hilton, or a massage and a nice espresso right before prayer time.\\n\\nThe Nobel committee felt obligated to choose Obama on the basis of his many achievements for peace, including comforting Israel by insisting their country be made safer by being partitioned for the Palestinians, back to the 1948 boundaries. Perhaps this is why 4% of Israelis believe Obama has been good for their country. Yet, everyone knows that if the Palestinians can only get their land back, they will be as peaceful as slugs on fresh lettuce in their regained homes.\\n\\nThe Nobel Committee was also appreciative of Obama\\xe2\\x80\\x99s wise silence in not responding too vocally to Iran, when the people unfairly arose and nonsensically insisted a perfectly honest election was illegitimate. Of course, the Iranian Government had no choice but to begin to call out snipers to help move back the very dangerous unarmed crowds of silent protesters. Imagine if Obama had visibly protested this dangerous almost-revolution? Tehran may have felt the need to launch a few of their new nuclear missile towards America to back us off. Fortunately, they can now save those precious nukes in case they need to help Israel get an attitude adjustment for their own dangerous policies.\\n\\nObama also had a wise reaction when he condemned the illegal coup launched in Honduras when a few yahoos (including the legislature and supreme court) kicked President Zelaya out of the country. The president had only attempted to pass out ballots using the army, printed up by Hugo Chavez, which would have allowed him to update the Constitution and become Honduran president for life. Obama saw this, and realized how this lawless reaction could undermine world authority, and wisely backed the Chavez faction. Obviously, the anti-imperialist book Hugo passed him, \\xe2\\x80\\x9cOpen Veins of Latin America: Five Centuries of the Pillage of a Continent\\xe2\\x80\\x9d was read, and it helped a chastened Obama become an even better president, which many historians had already stated was a scientific impossibility.\\n\\nThe Nobel Committee decided to award Barack Obama the Nobel Peace Prize based not only on his achievements, but his future potential. In this, Barack is being treated like a highly-drafted NFL quarterback rookie, given a multi-million dollar contract before throwing a single pass (the name Ryan Leaf comes to mind, of course). Listed as some of the good deeds Obama is expected by the Nobel Committee to tackle, are: Curing cancer; Delivering global health care while saving trillions; Replacing oil with the sun and building sunshine-driven cars; Moving Israel to a comfortable new settlement in Antarctica; Giving every child in the world a puppy and a lifetime dog food waiver; Digging up the objectionable corpse of Ronald Reagan, beheading and ritually burning it; Delivering a signed copy of das Kapital to every man, woman and child; Creating genuine fat loss in a single pill; Making even discussion of Capitalism or Adam Smith a felony; Using sociology to wipe out global nose-picking and other obnoxious habits; Getting rid of gravity in some vexing situations; Anathematizing John Locke; Creating a Hall of Fame for Community Organizers; and Serving upon those convicted of racially insensitive comments the death penalty, etc.\\n\\nWith leadership provided by the likes of G.I Joe and Barack Obama, America is on the verge of her greatest achievements. Undoubtedly, the coming years will only confirm Barack Obama\\xe2\\x80\\x99s status as greatest leader in history, finally nudging aside Caesar Augustus. Stay tuned, sportsfans\\xe2\\x80\\xa6'],\n","      dtype=object)>)])\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-235-3ef303cabb02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtrain_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;31m# Log every 50 batches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-235-3ef303cabb02>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtrain_acc_metric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\lolzc\\Desktop\\AICamp\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1072\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1075\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\lolzc\\Desktop\\AICamp\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\lolzc\\Desktop\\AICamp\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_aggregate_grads\u001b[1;34m(gradients)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m     assert all(isinstance(g, (ops.Tensor, ops.IndexedSlices))\n","\u001b[1;32mc:\\Users\\lolzc\\Desktop\\AICamp\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36madd_n\u001b[1;34m(inputs, name)\u001b[0m\n\u001b[0;32m    448\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    451\u001b[0m         _ctx, \"AddN\", name, inputs)\n\u001b[0;32m    452\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["## Note: Rerunning this cell uses the same model variables\n","model = newsNet\n","\n","def process(batch):\n","    #     OrderedDict(\n","    #   [('content', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'TExt'], dtype=object)>)],\n","    #   [('content', <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'TExt'], dtype=object)>)])\n","    return tf.constant([[[w.vector for w in text_to_nlp(str(text)) if not (w.is_stop or w.lemma_ =='-PRON-' or w.is_punct)] for text in batch['content'].numpy()][0]], dtype=np.float32)\n","#@tf.function(input_signature=(tf.TensorSpec(shape=[1, None, 300], dtype=np.float32), tf.TensorSpec(shape=(1, len(classes)), dtype=np.float32)))\n","def train_step(x, y):\n","    with tf.GradientTape() as tape:\n","        logits = model(x, training=True)\n","        loss_value = loss_fn(y, logits)\n","    grads = tape.gradient(loss_value, model.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","    train_acc_metric.update_state(y, logits)\n","    return loss_value\n","\n","\n","train_acc_metric = keras.metrics.CategoricalAccuracy()\n","val_acc_metric = keras.metrics.CategoricalAccuracy()\n","#@tf.function(input_signature=(tf.TensorSpec(shape=[1, None, 300], dtype=np.float32), tf.TensorSpec(shape=(1, len(classes)), dtype=np.float32)))\n","def test_step(x, y):\n","    val_logits = model(x, training=False)\n","    val_acc_metric.update_state(y, val_logits)\n","\n","\n","# Keep results for plotting\n","train_loss_results = []\n","train_accuracy_results = []\n","\n","import time\n","epochs = 3\n","for epoch in range(epochs):\n","    print(\"\\nStart of epoch %d\" % (epoch,))\n","    start_time = time.time()\n","    # Iterate over the batches of the dataset.\n","    step = 0\n","    for batch_data, label in dataset:\n","        if (label.numpy()[0] not in hot):\n","            step += 1\n","            continue\n","        if step < train_len:\n","            loss_value = train_step(process(batch_data), hot[label.numpy()[0]])\n","\n","            # Log every 50 batches.\n","            if step % 50 == 0:\n","                print(\n","                    \"Training loss (for one batch) at step %d: %.4f\"\n","                    % (step, float(loss_value)))\n","        else:\n","            if step == train_len:\n","                # Display metrics at the end of each epoch.\n","                train_acc = train_acc_metric.result()\n","                print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n","\n","                # Reset training metrics at the end of each epoch\n","                train_acc_metric.reset_states()\n","            test_step(process(batch_data), hot[label.numpy()[0]])\n","        step += 1\n","\n","    val_acc = val_acc_metric.result()\n","    val_acc_metric.reset_states()\n","    print(\"Validation acc: %.4f\" % (float(val_acc),))\n","    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n","    step=0\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["idx = np.random.randint(0, len(true_data))\n","\n","res = model(tf.constant([[x.vector for x in text_to_nlp(true_data.at[idx, 'text'])]]))\n","print(res[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the model\n","model.save('news_classifier') "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["step = 0\n","for text, fake in tqdm.tqdm(zip(x_test, y_test)):\n","    x_batch_val = tf.constant([[x.vector for x in text_to_nlp(text)]])\n","    y_batch_val = np.reshape([fake], newshape=(1,1))\n","    test_step(x_batch_val, y_batch_val)\n","    step += 1\n","    if step % 100 == 0:\n","        print_remove(\"Validation acc: %.4f\" % (float(val_acc_metric.result()),))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_acc = val_acc_metric.result()\n","print_remove(\"Validation acc: %.4f\" % (float(val_acc),))\n","print_remove(\"Time taken: %.2fs\" % (time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_acc_metric.reset_states()"]},{"cell_type":"markdown","metadata":{"id":"WchEBxFeA46-"},"source":["## Exercise 4 \n","\n"]},{"cell_type":"code","metadata":{"id":"xv_PHkE6346w"},"source":["# train_y_pred = model.predict([[x.vector for x in text_to_nlp(y)] for y in x_test])\n","# print('Train accuracy', accuracy_score(y_test, train_y_pred))\n","\n","for text, fake in tqdm.tqdm(zip(x_test, y_test)):\n","    x_batch_val = tf.constant([[x.vector for x in text_to_nlp(text)]])\n","    y_batch_val = np.reshape([fake], newshape=(1,1))\n","    test_step(x_batch_val, y_batch_val)\n","\n","val_acc = val_acc_metric.result()\n","val_acc_metric.reset_states()\n","print_remove(\"Validation acc: %.4f\" % (float(val_acc),))\n","print_remove(\"Time taken: %.2fs\" % (time.time() - start_time))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ylD5OWTZoCz2"},"source":["We can see that we are not doing very well, but we are doing better than 50%. We can do the same for the val data to see how we are doing on unseen data, which is more valuable for us if we want to make predictions on new websites. Fill in the code below to evaluate val accuracy!"]},{"cell_type":"markdown","metadata":{"id":"Hq6sGGVHobjo"},"source":["We appear to be doing a bit worse on the val data, not much better than chance. To better understand the performance of our binary classification model, we should seek to better understand the mistakes that it is making. Specifically, when our model makes a mistake (about 40% of the time), are these mistakes false negatives or false positives?"]},{"cell_type":"markdown","metadata":{"id":"0A_HLeiz9IlH"},"source":["## Confusion Matrix "]},{"cell_type":"markdown","metadata":{"id":"VeWbCvMa8rhg"},"source":["To answer these questions, we produce and analyze the confusion matrix. The confusion matrix is a matrix that shows the following:\n","\n","![Confusion Matrix](https://cdn-images-1.medium.com/max/1600/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n","\n","where the terms mean\n","\n","* TP (True Positive) = You predicted positive (fake in our case, since fake has a label of 1) and it’s true.\n","* FP (False Positive) = You predicted positive and it’s false.\n","* FN (False Negative) = You predicted negative and it’s false.\n","* TN (True Negative) = You predicted negative and it’s true.\n","\n","\n","###Common Metrics\n","\n","From the confusion matrix, we can extract commonly used metrics like precision (TP/(TP + FP)) and recall (TP/(TP + FN)). \n","\n","* Precision quantifies how often the things we classify as positive are actually positive. For our task, this measures what fraction of the sites we classify as fake are actually fake. \n","* Recall quantifies what fraction of actually positive examples we classify as positive. In our case, this is the fraction of fake news websites that we actually identify as fake.\n","\n","Finally, a useful score to summarize both precision and recall is the F-1 score. This is just a simple function (the harmonic mean) of precision and recall, shown in the summary below:\n","\n","<img src=\"https://datascience103579984.files.wordpress.com/2019/04/capture3-24.png\" width=\"400\" height=\"200\"></img>"]},{"cell_type":"markdown","metadata":{"id":"pftTwsCvE8Lt"},"source":["##Exercise 5 |  Using the Confusion Matrix "]},{"cell_type":"markdown","metadata":{"id":"tc9BxsQ-GYOo"},"source":["Run the cell below to create the confusion matrix for our own model. "]},{"cell_type":"code","metadata":{"id":"M9aYm4oqsL6P"},"source":["print('Confusion matrix:')\n","print(confusion_matrix(y_train, train_y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q5Of0ZHsHZ0C"},"source":["A Confusion Matrix can quickly tell you how well your model is doing. The primary way to figure this out is to calculate the Error Rate. \n","\n","The Error Rate is:   (FP) + (FN)) / (TP + FP + FN + TN).\n","\n","This is just all the false predictions (False Negative + False Positive) divided by all the predictions added together.  \n","\n","Use the Confusion Matrix we just created to calculate the Error Rate for our model. "]},{"cell_type":"code","metadata":{"id":"Bo0Bej9z_xcq"},"source":["### YOUR CODE HERE ###\n","\n","### END CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ac9XXM9XsU0"},"source":["## Exercise 6"]},{"cell_type":"code","metadata":{"id":"gVxJhwhzspF-"},"source":["print(val_y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nh-qi0tvGvFH"},"source":["We can see that we have many false negatives, and not as many false positives. Why is this the case? If we print out *val_y_pred*, we can see that our model is mostly predicting 0's (websites are real)."]},{"cell_type":"markdown","metadata":{"id":"fa5GCh7eG7OU"},"source":["What fraction of predictions in *val_y_pred* are 1's? Hint: you may find *np.mean* useful."]},{"cell_type":"code","metadata":{"id":"pStoI4cLfKDv"},"source":["### YOUR CODE HERE ###\n","\n","### END CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQdhOeigtmCp"},"source":["Why so many 0's? The only information we are giving our model is its domain name extension. It's natural that the model would learn that websites with \".biz\" extensions are unlikely to be reliable news websites, but it is still the case that most websites in the dataset (fake and real) have \".com\" extensions. Thus, our model will misclassify many fake news websites with \".com\" extensions as real. "]},{"cell_type":"code","metadata":{"id":"tHQTv6L98uno"},"source":["prf = precision_recall_fscore_support(val_y, val_y_pred)\n","\n","print('Precision:', prf[0][1])\n","print('Recall:', prf[1][1])\n","print('F-Score:', prf[2][1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vw9xDPTsruaB"},"source":["Again, the precision and recall metrics suggest that when we classify a website as fake, we are usually right, but we are not doing great at classifying these websites as fake frequently enough."]},{"cell_type":"markdown","metadata":{"id":"vyJC74B7ALAy"},"source":["##Using Keywords for a Stronger Baseline "]},{"cell_type":"markdown","metadata":{"id":"_3lY7pMkAPpX"},"source":["The key problem with our model in its current state is that it simply does not have enough information. This should not be a surprise–it was pretty unlikely in the first place that domain name extensions would be enough. If you like, feel free to add a few more extensions in the “featurizer” above and re-run all the code for evaluation–you'll find it doesn't make much of a difference.\n","Where can we get more information about webpages? From the HTML! Remember that the HTML contains all of the text and structure of a webpage. If we cleverly choose features from the HTML to feed into our logistic regression model, we will drastically improve our performance. We saw yesterday that probing hypotheses related to the counts of hypotheses words produced interesting results, and we will continue in this direction today to produce a model that leverages these differences in word frequencies.\n"]},{"cell_type":"markdown","metadata":{"id":"TyRp6ooJAnng"},"source":["## Exercise 7: Instructor-Led Discussion on Better Input Features\n"]},{"cell_type":"markdown","metadata":{"id":"L4jjZ5HF35K0"},"source":["\n","The below code introduces a better featurizer that counts the number of keywords (normalized using the *log* function) in the HTML. Normalizing the counts is a trick that prevents the featurized values from becoming too extreme. Read the code and make sure you understand what it is doing. Then add \"sports\" and \"finance\" as additional keywords to expand our model.\n","\n","**Run the below code and discuss what it is doing as a class. Add in additional keywords to further expand our model as you see fit.**\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sk_BBflEAuX3"},"source":["##Exercise 8"]},{"cell_type":"markdown","metadata":{"id":"bgWx3s7UxDue"},"source":["\n","\n","Let's run and evaluate the above featurizer. Add in code to fit the model, compute train accuracy, val accuracy, val confusion matrix, and val precision, recall, and F1-Score, just as before."]},{"cell_type":"code","metadata":{"id":"fxEIPqn1f1iQ"},"source":["train_X, train_y, feature_descriptions = prepare_data(train_data, keyword_featurizer)\n","val_X, val_y, feature_descriptions = prepare_data(val_data, keyword_featurizer)\n","\n","print('Number of features per example:', len(train_X[0]))\n","print('Feature descriptions:')\n","print(feature_descriptions)\n","print()\n","  \n","baseline_model = LogisticRegression()\n","\n","### YOUR CODE HERE ###\n","\n","### END CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1l9Jq6CzwWOt"},"source":["## Interpreting our Model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_hTNITBgB871"},"source":["### Instructor-Led Discussion: Interpreting Input Variables"]},{"cell_type":"markdown","metadata":{"id":"43z3A67xB4Qu"},"source":["As mentioned earlier, a key motivation for using a simpler model is interpretability.\n","\n","We've learned that the prediction of a logistic regression classifier is just the output of a multiplication with model weights, followed by a non-linear transformation (sigmoid). Because the sigmoid function is always increasing (monotonic) on its domain (see below), we know that if the dot product (or multiplication of vectors) between model weights and input features is large, then the output prediction will be closer to 1. If the dot product is small, then the output prediction will be closer to 0.\n","\n","![Sigmoid](https://cdn-images-1.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png)\n","\n","Thus, the weights corresponding to features tell us whether the features are important in the classification. If the weight corresponding to the feature \".net domain\" has a large positive value, then websites with \".net\" domains are more likely to be classified as fake (since fake has label 1). If it has a large negative value, then these websites are more likely to be classified as real. If it has value close to 0, then the feature may not be useful (at least, it may not be useful given that the other features are present).\n"]},{"cell_type":"markdown","metadata":{"id":"nqIKnF6fCgnP"},"source":["###Using Feature Descriptions"]},{"cell_type":"markdown","metadata":{"id":"0G8LwtG2Cj9u"},"source":["Let's see what weights our model learned. The code below uses *feature_descriptions* and the weights, or coefficients, of the model and sorts them in ascending order."]},{"cell_type":"code","metadata":{"id":"QHcJkqd4zic8"},"source":["sorted(zip(feature_descriptions, baseline_model.coef_[0].tolist()), key=lambda x: x[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qfIAQeg5zpRL"},"source":["## Exercise 9\n","\n","Answer the following questions:\n","\n","* What features have positive weight (most predictive of being fake)? What does that indicate?\n","* Which ones have negative weight (most predictive of being real)? What does that indicate?\n","* Which ones have close to 0 weight? \n","* Are there any feature weights that surprise you? \n","* Try coming up with explanations for why the feature weights are the way they are. Does this help you come up with new feature ideas? (~15 minutes)"]},{"cell_type":"code","metadata":{"id":"gnYk7Vp_YiSO"},"source":["'''\n","- \n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"082Dut9agst6"},"source":["## Instructor-Led Discussion: Final Interpretation of Inputs"]},{"cell_type":"markdown","metadata":{"id":"gfQvDlNAYjAa"},"source":["##Exercise 10 |  Final Baseline\n","\n","Finally, play around with the last few cells, adding more keywords and domain names to see how the results change. Note that \"keywords\" can be a variety of things: English words, English phrases (spaces are allowed), HTML tags, and any other string present in HTML. Also notice how the weights on different features vary–you may observe some interesting effects. When you are done, run the cell below to run evaluations again!"]},{"cell_type":"code","metadata":{"id":"19rbiCbuP8iq"},"source":["train_y_pred = baseline_model.predict(train_X)\n","print('Train accuracy', accuracy_score(train_y, train_y_pred))\n","\n","val_y_pred = baseline_model.predict(val_X)\n","print('Val accuracy', accuracy_score(val_y, val_y_pred))\n","\n","print('Confusion matrix:')\n","print(confusion_matrix(val_y, val_y_pred))\n","\n","prf = precision_recall_fscore_support(val_y, val_y_pred)\n","\n","print('Precision:', prf[0][1])\n","print('Recall:', prf[1][1])\n","print('F-Score:', prf[2][1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6aOTjSX0_yj"},"source":["Congratulations on completing this notebook. Looking at the results of our final baseline, you may be surprised this approach is working at all–after all, our model is still barely looking at the content of websites. We will further explore the issue of modeling the content of websites tomorrow, but as a result of our efforts today, we now know that we can make progress with a relatively simple approach!"]}]}