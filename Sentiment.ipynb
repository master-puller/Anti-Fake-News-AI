{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"InstructorSolution_DoNotEdit_Yelp_Review_Sentiment_Classification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"svLhCiU3Evm6"},"source":["# Introduction to Yelp Review Sentiment Classification\n","## Double-click the \"Solutions hidden\" cells to see the answers\n","\n","In this project, we will build a classifier that can predict a user's rating of a given restaurant from their review. This is an example of **sentiment analysis**: being able to quantify an individual's opinion about a particular topic merely from the words they use. \n","\n","**Discuss:** Can you think of other ways companies might use sentiment analysis?\n","\n","In this notebook, we'll:\n","\n","\n","*   Explore the Yelp review dataset\n","*   Preprocess and vectorize our text data for NLP\n","*   Train a sentiment analysis classifier with logistic regression\n","*   (Optional) Explore and improve our model\n","*   (Optional, Advanced) Train a model with word embeddings\n","*   (Optional, Challenge) Use word embeddings to calculate similarity and analogies\n","\n","Let's dive in!\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BZgELVS8I6kc"},"source":["![Example of a Yelp review](https://wordstream-files-prod.s3.amazonaws.com/s3fs-public/styles/simple_image/public/images/yelp-reviews-filtered.png)"]},{"cell_type":"code","metadata":{"cellView":"form","id":"2jS5ThMCEvnC"},"source":["#@title Import our libraries (this may take a minute or two)\n","import pandas as pd   # Great for tables (google spreadsheets, microsoft excel, csv). \n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import string\n","import nltk\n","import spacy\n","import wordcloud\n","import os # Good for navigating your computer's files \n","import sys\n","pd.options.mode.chained_assignment = None #suppress warnings\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from spacy.lang.en.stop_words import STOP_WORDS\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import accuracy_score\n","!python -m spacy download en_core_web_md\n","import en_core_web_md\n","text_to_nlp = en_core_web_md.load()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KsF8tioZLicU","cellView":"form"},"source":["#@title Import our data\n","\n","# import gdown\n","#gdown.download('https://drive.google.com/uc?id=1u0tnEF2Q1a7H_gUEH-ZB3ATx02w8dF4p', 'yelp_final.csv', True)\n","data_file  = 'yelp_final.csv'\n","\n","!wget https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%203%20-%20NLP/yelp_final.csv\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DQ267zCBOjet"},"source":["# Data Exploration"]},{"cell_type":"markdown","metadata":{"id":"_BLs_2JkEvnw"},"source":["First we read in the file containing the reviews and take a look at the data available to us."]},{"cell_type":"code","metadata":{"id":"8dZ_lymcN_K9"},"source":["# read our data in using 'pd.read_csv('file')'\n","yelp_full = pd.read_csv(data_file)\n","yelp_full.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QjL5FrSLEvoP"},"source":["**Discuss:**\n","- Which column gives us our output: the user's sentiment? \n","- Which column gives us our input: the review?\n","- Why aren't businesses' and users' real names included? (You'll notice they're replaced with random strings through [hashing](https://medium.com/tech-tales/what-is-hashing-6edba0ebfa67)). Why aren't the real names included?\n","\n","Let's keep only the two columns we need:\n","\n"]},{"cell_type":"code","metadata":{"id":"pB1yKcUtcpg9"},"source":["needed_columns = [] #YOUR CODE HERE: fill in the columns\n","yelp = yelp_full[needed_columns]\n","yelp.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8T93-bsPHalS"},"source":["#@title Instructor Solution\n","needed_columns = ['stars','text']\n","yelp = yelp_full[needed_columns]\n","yelp "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MExj8roOEvog"},"source":["The text column is the one we are primarily focused with. Let's take a look at a few of these reviews to better understand our problem."]},{"cell_type":"code","metadata":{"cellView":"form","id":"la3rUPKgEvoi"},"source":["#@title Check out the text in differently rated reviews\n","num_stars =  1#@param {type:\"integer\"}\n","\n","for t in yelp[yelp['stars'] == num_stars]['text'].head(20).values:\n","    print (t) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0GZ6NN4yEvos"},"source":["**Discuss:**\n","\n","\n","*   What words are often used in highly rated reviews?\n","*   What words are often used in low-rated reviews?\n","*   Do you notice any interesting exceptions? (For example, \"The seating and ambience were impressive, but the food served to us was not\")\n"]},{"cell_type":"markdown","metadata":{"id":"TQHod14KEvoz"},"source":["#### Word Clouds\n","\n","Another way to take a look at the most prominent words in any given star rating is through the use of word clouds. "]},{"cell_type":"markdown","metadata":{"id":"oHY5IhnKEvo8"},"source":["Edit the value in the cell below to see the word cloud for each star rating."]},{"cell_type":"code","metadata":{"cellView":"form","id":"FtMnf1zLEvo_"},"source":["#@title Word cloud for differently rated reviews\n","num_stars =  1#@param {type:\"integer\"}\n","this_star_text = ''\n","for t in yelp[yelp['stars'] == num_stars]['text'].values: # form field cell\n","    this_star_text += t + ' '\n","    \n","wordcloud = WordCloud()    \n","wordcloud.generate_from_text(this_star_text)\n","plt.figure(figsize=(14,7))\n","plt.imshow(wordcloud, interpolation='bilinear')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nn4v3upxEvpL"},"source":["**What are the differences between the reviews that have 1, 2, 3, 4, and 5 stars?**\n","\n","Any surprising similarities? \n"]},{"cell_type":"markdown","metadata":{"id":"ZpU7nrWTEvov"},"source":["### Exercise: Rules for Sentiment Analysis\n","\n","Can you think of any combinations of words, or rules, that would indicate if a particular review is **postive** or **negative**? Note them down below:\n","\n","\n"]},{"cell_type":"code","metadata":{"cellView":"form","id":"dxuZyKOKy6Cc"},"source":["#@title Rules\n","rule_1 = \"\" #@param {type:\"string\"}\n","rule_2 = \"\" #@param {type:\"string\"}\n","rule_3 = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J4PQg8FhEvow"},"source":["**Discuss: will rules like this work well?**\n","\n","It might not be enough to just see **whether** particular words are used! We also should look at **how much** they're used: the *number* of particular words might give us information about the user's opinion.\n"]},{"cell_type":"markdown","metadata":{"id":"dArbYofKN206"},"source":["# Preparing Our Data for Machine Learning\n","\n","Of course, it's much more efficient to use machine learning to analyze our text than try to create rules by hand! \n","\n","We'll need to prepare our data to use logistic regression. First, let's prepare our output column:"]},{"cell_type":"markdown","metadata":{"id":"0bwasn11BfD4"},"source":["### Exercise: Preparing to Classify\n","We're going to try to predict the sentiment - **positive** or **negative** - based on a review's text. \n","\n","In order to reduce our problem to a **binary classification** (two classes) problem, we will:\n","\n"," - label 4 and 5 star reviews as 'good'\n"," - label 1, 2, 3 star reviews as 'bad'\n","\n","Please complete the function below and run it to create a new `is_good_review` column!"]},{"cell_type":"code","metadata":{"id":"ck4iX6PITzHS"},"source":["def is_good_review(num_stars):\n","    if None: ### YOUR CODE HERE\n","        return True\n","    else:\n","        return False\n","\n","# Change the stars column to either be 'good' or 'bad'.\n","yelp['is_good_review'] = yelp['stars'].apply(is_good_review)\n","yelp.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"x2uO1pbAEvrI"},"source":["#@title Instructor Solution\n","def is_good_review(num_stars):\n","    if num_stars > 3: ### YOUR CODE HERE\n","        return True\n","    else:\n","        return False\n","\n","yelp['is_good_review'] = yelp['stars'].apply(is_good_review)\n","yelp.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W8raBKzTEvpM"},"source":["## Text Preprocessing: A Preview\n","\n","Now, the trickier part: preparing our text input.\n","\n","We'll need a few steps to preprocess our text and represent it numerically. **Why do we need to represent our text as numbers?**\n","\n","We'll talk through all the steps here, then use a single function to implement them."]},{"cell_type":"markdown","metadata":{"id":"pby7LlwhEvpN"},"source":["## Tokenization\n","\n","First of all, we would like to **tokenize** each review: convert it from a single string into a list of words. Enter some example text into the cell below to see the tokenized version."]},{"cell_type":"code","metadata":{"id":"7XOaa1uEEvpY"},"source":["#@title Basic tokenization example\n","example_text = \"All the people I spoke to were super nice and very welcoming.\" #@param {type:\"string\"}\n","tokens = word_tokenize(example_text)\n","tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QKCP5Q_LEvpg"},"source":["## Stopwords\n","\n","Next, let's remove **stopwords**: words which are there to provide grammatical structure, but don't give us much information about a review's sentiment.\n","\n","Edit the cell below to see if we're considering a given word as a stopword! Do you agree with the results?"]},{"cell_type":"code","metadata":{"id":"Nqq4w-ZrEvpj"},"source":["#@title Check if a word is a stop word\n","example_word = \".\" #@param {type:'string'}\n","if example_word.lower() in STOP_WORDS:\n","  print ('\"' + example_word + '\" is a stop word.')\n","else:\n","  print ('\"' + example_word + '\" is NOT a stop word.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vubl4ejEvpv"},"source":["We're going to remove these stopwords from the user reviews.\n","\n","Tokenization and removal of stop words are universal to nearly every NLP application. In some cases, additional cleaning may be required (for example, removal of proper nouns, removal of digits) but we can build a text preprocessing function with these \"base\" cleaning steps.\n","\n","Putting all these together, we can come up with a text cleaning function that we can apply to all of our reviews."]},{"cell_type":"markdown","metadata":{"id":"XfIUrRtEWr3H"},"source":["## Vectors\n","\n","Finally, we'll need to convert our text to **vectors**, or lists of numbers. We'll start off doing this with Bag of Words, but we'll talk about another approach later!\n"]},{"cell_type":"markdown","metadata":{"id":"43hbR0vha7E3"},"source":["### Bag of Words\n","\n","In a **bag of words** approach, we count how many times each word was used in each review.\n","\n","Suppose we want to represent two **reviews**: \n","- \"The food was great. The ambience was also great.\"\n","- \"Great ambiance, but not great food!\"\n","\n","First we define our vocabulary. This is *each unique word* in the review. So our **vocabulary** is: \n","- [also, ambience, but, food, great, not, the, was].\n","\n","Next, we count up how many times each word was used! (You can also think of this as adding up one-hot encodings.)\n","\n","Our reviews are encoded as:\n","- **First review:** [1, 1, 0, 1, 2, 0, 2, 2]. Can you explain why?\n","- **Second review:** [_, _, _, _, _, _, _, _] Fill it in here! \n","\n"]},{"cell_type":"markdown","metadata":{"id":"NbFU78B5bXka"},"source":["## Preprocessing Our Text in Action\n","\n","Let's use bag-of-words to prepare our data! \n","\n","First, let's select our input *X* and output *y*:"]},{"cell_type":"code","metadata":{"id":"6t6HQm1vEvrQ"},"source":["X_text = yelp['text']\n","y = yelp['is_good_review']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xhRyg_YEeA5t"},"source":["Now, let's prepare our data! First, we'll use CountVectorizer, a useful tool from Scikit-learn, to: \n","*   Tokenize our reviews\n","*   Remove stopwords\n","*   Prepare our vocabulary\n"]},{"cell_type":"code","metadata":{"id":"yrSQAeKjAiXJ"},"source":["#@title Initialize the text cleaning function { display-mode: \"form\" }\n","def tokenize(text):\n","    clean_tokens = []\n","    for token in text_to_nlp(text):\n","        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct): # -PRON- is a special all inclusive \"lemma\" spaCy uses for any pronoun, we want to exclude these \n","            clean_tokens.append(token.lemma_)\n","    return clean_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NfDtH-XTSrCK"},"source":["The cell below will take a moment! **Can you guess what `max_features` is for?**\n"]},{"cell_type":"code","metadata":{"id":"blZ7RJ2zEvrU"},"source":["bow_transformer = CountVectorizer(analyzer=tokenize, max_features=800).fit(X_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RaheGj_RmKW7"},"source":["Now, we can see our entire vocabulary! Can you guess what the numbers represent?"]},{"cell_type":"code","metadata":{"id":"6TjdgYVxmKgd"},"source":["bow_transformer.vocabulary_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nGEf8h1lEvrY"},"source":["The number represents the **index** (alphabetical position) of a word in the vocabulary.\n","\n","By the way, how many words do we have?\n"]},{"cell_type":"code","metadata":{"id":"upN1gxm5Evrb"},"source":["len(bow_transformer.vocabulary_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qkN4E-QCTFom"},"source":["It's the same as `max_features`! Is that a coincidence? What's the point of `max_features`?"]},{"cell_type":"markdown","metadata":{"id":"YsFa7Nu5Evr4"},"source":["Now that our vocabulary is ready, we can **transform** each review into a bag of words.\n","\n"]},{"cell_type":"code","metadata":{"id":"VRJJe2HGEvr6"},"source":["X = bow_transformer.transform(X_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pu6FEly_UMFC"},"source":["Finally, we've converted our reviews to numerical data that we can use in a logistic regression!\n","\n","We can see what `X` looks like by printing it out as a DataFrame. **How long is each review's vector?** Why?"]},{"cell_type":"code","metadata":{"id":"FdeKo8ZQTiOu"},"source":["pd.DataFrame(X.toarray())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6mLW7krRcBIG"},"source":["## Interlude: Word Embeddings\n","\n","By the way, there's another way we could have converted our text to vectors: using **word embeddings** like Word2Vec. \n","\n","Let's explore word embeddings using a library called Spacy, which comes built-in with lots of useful information about the English language. Let's prepare a useful function from Spacy:"]},{"cell_type":"code","metadata":{"id":"Nuu0QFg-VOiN"},"source":["text_to_nlp = en_core_web_md.load() #Prepare Spacy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eYov_nu-VXQn"},"source":["`text_to_nlp` lets us find lots of information about a sentence. For example, we can pick out a specific word:"]},{"cell_type":"code","metadata":{"id":"ldEPkz8NcI6_"},"source":["doc = text_to_nlp(u\"I like apples and cherries and peaches and pie\")\n","token = doc[2] #Try changing this!\n","print (token)\n","print (len(token))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EWyXczXkePiI"},"source":["We can also find the **word embedding** for each word: a 300-dimensional vector that captures the word's meaning!"]},{"cell_type":"code","metadata":{"id":"E0tBmojQeNhn"},"source":["print ('Vector for: ', token)\n","print (token.vector) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JBagECaVMQXZ"},"source":["#### Exercise: Exploring Similarity\n","\n","A neat thing you can do with word embeddings is calculate similarity, like this:\n","\n","```\n","doc = text_to_nlp(u\"keyboard and mouse\")\n","word1 = doc[0]\n","word2 = doc[2]\n","word1.similarity(word2)\n","```\n","\n","Using the example above, try to find **two words with similarity greater than .80** and **two words with similarity less than .15**!"]},{"cell_type":"code","metadata":{"id":"ZD8fVPwlM4Bn"},"source":["### YOUR CODE HERE\n","\n","### END CODE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E1myx2wBXQmH"},"source":["Check out the challenge exercises if you're interested in exploring word embeddings further with our Yelp dataset."]},{"cell_type":"markdown","metadata":{"id":"P8AUxyLHEvr_"},"source":["# Creating a Baseline Classifier\n","\n","Now, back to our sentiment analysis problem! Our data is ready for machine learning.\n","\n","Our classification problem is a classic two-class classification problem, and so we will use the tried-and-tested **Logistic Regression** machine learning model.\n","\n","As always, we'll start by setting aside testing and training data:"]},{"cell_type":"code","metadata":{"id":"PThy6pNUEvsA"},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42rl41sUXr0-"},"source":["###Exercise: Training your Model\n","Now, we can create and train our model! Please **fill in the code to train (or *fit*) your model**.\n","\n","(Need a hint? Refer to last time's notebook or Scikit-learn documentation if needed.)"]},{"cell_type":"code","metadata":{"id":"gIpbdNZwgRTn"},"source":["logistic_model = LogisticRegression()\n","\n","#YOUR CODE HERE to train the model (1 line)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQNdY8k8X6bq"},"source":["#@title Instructor Solution\n","logistic_model = LogisticRegression()\n","logistic_model.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8hFMR7NUV84D"},"source":["###Exercise: Testing Your Model\n","Now, let's evaluate our model's accuracy! Your model needs to **predict** the sentiment, and then you'll **calculate the accuracy** using `accuracy_score`. **Which dataset** should you use?"]},{"cell_type":"code","metadata":{"id":"SVdf84tuWQwM"},"source":["y_pred = None #YOUR CODE HERE\n","accuracy = None #YOUR CODE HERE\n","print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E_0C2bHaV84F"},"source":["#@title Instructor Solution\n","y_pred = logistic_model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zct-JZPQaXno"},"source":["Congratulations - you've trained and tested your model! It's not perfect, but a whole lot better than a coin flip :)\n"]},{"cell_type":"markdown","metadata":{"id":"ILybPDLAaphw"},"source":["#Optional: Exploring Your Model\n","\n","Let's explore your model in more depth."]},{"cell_type":"markdown","metadata":{"id":"qiUFnTXrased"},"source":["###Exercise: Trying Out Reviews\n","\n","Accuracy only tells us so much! It's often useful to figure out **what sorts** of mistakes your model makes. \n","\n","Try enterning some reviews below and explore:\n","\n","*   What kind of reviews does your model classify correctly? For example, do long or short reviews work better?\n","*   What kind of reviews does your model get wrong? Does it understand sarcasm or other \"tricky\" language?\n","*   Does it seem like your model pays attention to particular words?\n","\n"]},{"cell_type":"code","metadata":{"cellView":"form","id":"euuR1VWWEvsX"},"source":["#@title Enter a review to see your model's classification\n","example_review = \"This restaurant sucks\" #@param {type:'string'}\n","prediction = logistic_model.predict(bow_transformer.transform([example_review]))\n","\n","if prediction:\n","  print (\"This was a GOOD review!\")\n","else:\n","  print (\"This was a BAD review!\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AV0Fe3GpHd2_"},"source":["###Exercise: Changing the Vocabulary Size\n"]},{"cell_type":"markdown","metadata":{"id":"m1i_LdIeHqgo"},"source":["Experiment with changing the `max_features` attribute when you created the bag of words model: the maximum size of the vocabulary. Then re-train your model (you might find \"Runtime > Run after\" useful.) \n","\n","Discuss: how does this change affect the accuracy of your model? Why?"]},{"cell_type":"markdown","metadata":{"id":"4k-2NwiLHL4w"},"source":["###Exercise: Using a Different Classifier\n","\n","We used logistic regression for our baseline model, but there are many other classifier models we could use!\n","\n","One common model is called Multinomial Naive Bayes. Naive Bayes uses Bayes' Theorem of probability to predict the class of new input data. The important assumption that Naive Bayes makes is that all the features are independent: the number of times a review uses \"potato\" is unrelated to the number of times a review uses \"server\". (Do you think this is an accurate assumption??)\n","\n","Let's build a model using a Naive Bayes classifier!"]},{"cell_type":"code","metadata":{"id":"xPg0Y7cjHH2c"},"source":["from sklearn.naive_bayes import MultinomialNB\n","nb_model = MultinomialNB()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5nqtNo_qIsbh"},"source":["We can train and generate predictions from this model in the same way we did for our Logistic Regression model. Try training this model on the same data and see if it performs better or worse than our logistic regression model. Then, evaluate the model accuracy as you did for the Logistic Regression classifier.\n","\n"]},{"cell_type":"code","metadata":{"id":"5q-UOCh0I0BR"},"source":["###YOUR CODE HERE####"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"uDjdKj3RIoD8"},"source":["#@title Solution hidden\n","nb_model.fit(X_train, y_train)\n","nb_preds = nb_model.predict(X_test) \n","accuracy = accuracy_score(y_test, y_pred)\n","print (\"The accuracy of the model is \" + str(accuracy*100) + \"%\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8TffS4NT-BSS"},"source":["Experiment with [other models](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) to try to get the highest accuracy!"]},{"cell_type":"code","metadata":{"id":"d2-jsJ5F-B-a"},"source":["###YOUR CODE HERE###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFeI61HGY2Kt"},"source":["# Optional (Advanced): Training Logistic Regression with Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"b9DUXQXSaMPU"},"source":["As we've discussed, there's an alternative to bag-of-words: we can use word embeddings to get more sophisticated representations of our reviews. Let's try it out!\n"]},{"cell_type":"markdown","metadata":{"id":"9_n7uJ20a_yH"},"source":["We'll use this helper function to remove stop words, pronouns, and punctuation, and convert each word to a spaCy object (we can use `token.vector` later)."]},{"cell_type":"code","metadata":{"id":"QAOs86fEQ_2l"},"source":["def tokenize_vecs(text):\n","    clean_tokens = []\n","    for token in text_to_nlp(text):\n","        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct): \n","          # -PRON- is a special all inclusive \"lemma\" spaCy uses for any pronoun, we want to exclude these \n","            clean_tokens.append(token)\n","    return clean_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0XqDYs_b4V8"},"source":["We want to represent each Yelp review with a vector. Since each review consists of multiple words, we want to find a way to create one vector for each review. \n","\n","Would adding the word vectors work? What about averaging? Which would be preferrable?\n","\n","Implement your solution below: convert our array of reviews into an array of vector representations of those reviews."]},{"cell_type":"code","metadata":{"id":"4tKKKLymMNsd"},"source":["X_word2vec = []\n","for text in X_text:\n","  review = tokenize_vecs(text) # returns cleaned list of spacy tokens\n","  #### YOUR CODE HERE\n","  \n","  \n","  #### END CODE\n","  \n","X_word2vec = np.array(X_word2vec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"URKBtiv7cmUG"},"source":["#@title Instructor Solution (Uses Mean of Word Vectors)\n","\n","X_word2vec = []\n","for text in X_text:\n","  review = tokenize_vecs(text) # returns cleaned list of spacy tokens\n","  review_vec = [0]*300\n","  for word in review:\n","    review_vec += word.vector\n","  review_vec = review_vec / len(review)\n","  X_word2vec.append(review_vec)\n","\n","X_word2vec = np.array(X_word2vec)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"frxz5RO5c0ft"},"source":["Now, test and train a logistic regression mode! Remember to create new training and testing data using `X_word2vec`."]},{"cell_type":"code","metadata":{"id":"0c7m639IGVuT"},"source":["#YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"IzEOX1q9GYSl"},"source":["#@title Instructor Solution\n","# import a fresh logistic regression model from scikit-learn\n","w2v_model = LogisticRegression()\n","\n","# train-test split\n","X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(X_word2vec, y, test_size=0.2, random_state=101)\n","\n","w2v_model.fit(X_train_word2vec, y_train_word2vec)\n","\n","w2v_preds = w2v_model.predict(X_test_word2vec) \n","accuracy = accuracy_score(y_test_word2vec, w2v_preds)\n","print (accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehdlMAFaHByC"},"source":["**Discuss:** Can you explain the results? "]},{"cell_type":"markdown","metadata":{"id":"zcqbg3NSNriB"},"source":["# Optional (Challenge): Word Embedding Math\n","\n","(Heads-up: this challenge section is math-heavy!)"]},{"cell_type":"markdown","metadata":{"id":"kTyeFBh0CbNO"},"source":["One reason text embeddings are cool is that we can use them to explore connections in meaning between different words, including calculating similarity between words and completing [analogies](http://bionlp-www.utu.fi/wv_demo/).\n","\n","We'll start by creating a dictionary containing the vectors for all the words in our vocabulary. We'll stick to the vocabulary above of 800 words from the Yelp reviews - if you want to use more words, change that number! "]},{"cell_type":"code","metadata":{"id":"cHlGBkBKoUPI"},"source":["vocab_dict = dict() #initialize dictionary\n","\n","for word in bow_transformer.vocabulary_:\n","    vocab_dict[word] = text_to_nlp(word).vector # What is the key? What is the value?\n","\n","for word, vec in vocab_dict.items(): # Iterating through the dictionary to print each key and value\n","  print ('Word: {}. Vector length: {}'.format(word, len(vec)))\n","\n","print()\n","print ('{} words in our dictionary'.format(len(vocab_dict)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WPCcIyTpDuab"},"source":["Next, let's calculate the similarity between two words, using their Word2Vec representations.\n","\n","A common way to calculate the similarity between two vectors is called *cosine similarity*. It depends on the angle between those two vectors when plotted in space. As an example, imagine we had two three-dimensional vectors:"]},{"cell_type":"code","metadata":{"id":"omAmAv88GZUp"},"source":["v0 = [2,3,1]\n","v1 = [2,4,1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"owbBQZUgGgjs"},"source":["Run the code below to plot those vectors, and try changing the numbers above.\n","How can you make a very small angle between the vectors? How can you make a very large angle?"]},{"cell_type":"code","metadata":{"cellView":"form","id":"QtbbBLgcFmE0"},"source":["#@title Run this to create an interactive 3D plot\n","#Code from https://stackoverflow.com/questions/47319238/python-plot-3d-vectors \n","import numpy as np \n","import plotly.graph_objs as go\n","\n","def vector_plot(tvects,is_vect=True,orig=[0,0,0]):\n","    \"\"\"Plot vectors using plotly\"\"\"\n","\n","    if is_vect:\n","        if not hasattr(orig[0],\"__iter__\"):\n","            coords = [[orig,np.sum([orig,v],axis=0)] for v in tvects]\n","        else:\n","            coords = [[o,np.sum([o,v],axis=0)] for o,v in zip(orig,tvects)]\n","    else:\n","        coords = tvects\n","\n","    data = []\n","    for i,c in enumerate(coords):\n","        X1, Y1, Z1 = zip(c[0])\n","        X2, Y2, Z2 = zip(c[1])\n","        vector = go.Scatter3d(x = [X1[0],X2[0]],\n","                              y = [Y1[0],Y2[0]],\n","                              z = [Z1[0],Z2[0]],\n","                              marker = dict(size = [0,5],\n","                                            color = ['blue'],\n","                                            line=dict(width=5,\n","                                                      color='DarkSlateGrey')),\n","                              name = 'Vector'+str(i+1))\n","        data.append(vector)\n","\n","    layout = go.Layout(\n","             margin = dict(l = 4,\n","                           r = 4,\n","                           b = 4,\n","                           t = 4)\n","                  )\n","    fig = go.Figure(data=data,layout=layout)\n","    fig.show()\n","\n","\n","vector_plot([v0,v1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7xDNfU3HUE3"},"source":["For our Word2Vec vectors, we can imagine doing the same thing in 300-dimensional space. Of course, it's much harder to plot that! [Here](https://projector.tensorflow.org/) is one representation that you can play around with.\n","\n","Then we find the cosine of the angle between the two vectors to get the similarity. \n","\n","If the vectors are exactly the same, the angle will be 0, so we get a similarity of $cos(0) = 1$.\n","\n","If the vectors are exactly opposite, the angle will be 180 degrees, so we get a similarity of $cos(180) = -1$.\n","\n","There's a useful [mathematical trick](https://www.mathsisfun.com/algebra/vectors-dot-product.html) to find the cosine similarity:\n","\n","![](https://wikimedia.org/api/rest_v1/media/math/render/svg/1d94e5903f7936d3c131e040ef2c51b473dd071d)\n","\n","Where $A_1, A_2, ..., A_{300}$ are the elements of the first vector and $B_1, B_2, ..., B_{300}$ are the elements of the second vector.\n","\n","Please implement cosine similarity below, and test it out using our 3-dimensional vectors from above. Do the results make sense?\n"]},{"cell_type":"code","metadata":{"id":"fAnA_KdoNqr-"},"source":["def vector_cosine_similarity(vec1,vec2):\n","  #YOUR CODE HERE\n","  pass\n","  #Return a number between -1 and 1\n","\n","print(vector_cosine_similarity(v0,v1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"nPf7SLCGJtBZ"},"source":["#@title Instructor Solution\n","def vector_cosine_similarity(vec1,vec2):\n","  #Assume vec1 and vec2 have the same size \n","\n","  #YOUR CODE HERE\n","  numerator = 0\n","  for i in range(len(vec1)):\n","    numerator += vec1[i]*vec2[i]\n","  mag1 = (sum(elem**2 for elem in vec1))**0.5\n","  mag2 = (sum(elem**2 for elem in vec2))**0.5\n","  similarity = numerator/(mag1*mag2)\n","  return similarity\n","\n","print(vector_cosine_similarity(v0,v1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJ2JQmZELItA"},"source":["Now, use your cosine similarity function to calculate the similarity between two words. Try out a few words from the dataset - what pairs of words can you find that are particularly similar or particularly dissimilar?"]},{"cell_type":"code","metadata":{"id":"KwC6EioNJHKR"},"source":["def word_similarity(word1, word2):\n","  #Should return a similarity between -1 and 1\n","  \n","  try:\n","    vec1 = vocab_dict[word1]\n","    vec2 = vocab_dict[word2]\n","\n","    #TODO: Fill in the return statement here\n","\n","  except KeyError:\n","    print ('Word not in dictionary')\n","\n","print(word_similarity('burger','steak'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"NmdssfuLLhvx"},"source":["#@title Instructor Solution\n","def word_similarity(word1, word2):\n","  #Should return a similarity between -1 and 1\n","  \n","  try:\n","    vec1 = vocab_dict[word1]\n","    vec2 = vocab_dict[word2]\n","    return vector_cosine_similarity(vec1,vec2)\n","\n","  except KeyError:\n","    print ('Word not in dictionary')\n","\n","print(word_similarity('burger','steak'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w8KKXG2BLz7K"},"source":["Now, we can use our functions above to find the *most* similar words to any particular word. \n","\n","`find_most_similar(start_vec)` should output the top 5 words whose vectors are most similar to start_vec, with their similarities. Please fill it in.\n"]},{"cell_type":"code","metadata":{"id":"mgNpY_wZpG5A"},"source":["def find_nearest_neighbor(word):\n","  try:\n","    vec = vocab_dict[word]\n","    find_most_similar(vec)\n","  except KeyError:\n","    print ('Word not in dictionary')\n","\n","def find_most_similar(start_vec):\n","  #Should print the top 5 most similar words to start_vec, and their similarities.,\n","  #Hint: use a for loop to iterate through vocab_dict.\n","  #Consider using a Pandas series.\n","\n","  #YOUR CODE HERE\n","  pass\n","  \n","find_nearest_neighbor('bagel')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4AgwsvmZOmdd"},"source":["#@title Instructor Solution\n","def find_nearest_neighbor(word):\n","  try:\n","    vec = vocab_dict[word]\n","    find_most_similar(vec)\n","  except KeyError:\n","    print ('Word not in dictionary')\n","\n","def find_most_similar(start_vec):\n","  #Should print the top 5 most similar words to start_vec, and their similarities.,\n","  #Hint: use a for loop to iterate through vocab_dict.\n","  #Consider using a Pandas series.\n","\n","  #YOUR CODE HERE\n","  similarity_series = pd.Series(np.nan, index = vocab_dict.keys())\n","  for word, vec in vocab_dict.items():\n","    similarity_series[word] = vector_cosine_similarity(start_vec, vec)\n","  similarity_series = similarity_series[similarity_series.notna()] #get rid of N/A\n","  five_most_similar = similarity_series.sort_values().tail()\n","  print (five_most_similar) #words and similarities\n","\n","find_nearest_neighbor('bagel')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rxcyY1YZO9u5"},"source":["Finally, we can use the functions we've built to complete word analogies, like the ones you can try out [here](http://bionlp-www.utu.fi/wv_demo/). For example:\n","\n","*   Breakfast is to bagel as lunch is to ________,\n","\n","This requires a bit of \"word arithmetic\":\n","let's say A1, A2, and B1 are vectors for three words we know. We're trying to find B2 to complete \n","\n","*   A1 is to A2 as B1 is to B2.\n","\n","Intuitively, this means that the difference between A1 and A2 is the same as the difference between B1 and B2. So we write\n","\n","*   A1 - A2 = B1 - B2\n","\n","**Solve for B2:**\n","\n","*   B2 = ________________\n","\n","Once we know the vector that we \"expect\" for B2, we can use our previous functions to find the word whose representation is closest to that vector. Try it out!"]},{"cell_type":"code","metadata":{"id":"QfTeKGqYLw_1"},"source":["def find_analogy(word_a1, word_a2, word_b1):\n","  #Convert the words to vectors a1, a2, b1\n","  #If word_a1:word_a2 as word_b1:word_b2, then \n","  #a1 - a2 = b1 - b2\n","  #So b2 = ...\n","  #Calculate b2, and use your previous functions to find the best candidates for word_b2.\n","\n","  #YOUR CODE HERE\n","  pass\n","  \n","find_analogy('breakfast','bagel','lunch')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"gr88Et0_Q55g"},"source":["#@title Instructor Solution\n","def find_analogy(word_a1, word_a2, word_b1):\n","  #Convert the words to vectors a1, a2, b1\n","  #If word_a1:word_a2 as word_b1:word_b2, then \n","  #a1 - a2 = b1 - b2\n","  #So b2 = ...\n","  #Calculate b2, and use your previous functions to find the best candidates for word_b2.\n","\n","  #YOUR CODE HERE\n","  a1_vec = vocab_dict[word_a1]\n","  a2_vec = vocab_dict[word_a2]\n","  b1_vec = vocab_dict[word_b1]\n","  find_most_similar(b1_vec - a1_vec + a2_vec)\n","\n","find_analogy('breakfast','bagel','lunch')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zlDcBn9kQxNR"},"source":["Word arithmetic doesn't always work perfectly - it's pretty tricky to find good examples! Which can you discover?\n","\n","If you're looking for a way to expand further on this exercise, you can try seeing what happens when you use [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance), another common measurement, instead of cosine similarity."]}]}